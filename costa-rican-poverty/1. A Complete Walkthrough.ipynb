{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cbb767a6ce7a389511a40311cd4f01f1b09ad270"
   },
   "source": [
    "# Costa Rican Household Poverty Level Prediction\n",
    "\n",
    "\n",
    "Welcome to another Kaggle challenge! The objective of the Costa Rican Household Poverty Level Prediction contest is to develop a machine learning model that can predict the poverty level of households using both individual and household characteristics. This \"data science for good\" project offers the opportunity to put our skills towards a task more beneficial to society than getting people to click on ads!\n",
    "\n",
    "In this notebook, we will walk through a complete machine learning solution: first, get introduced to the problem, then perform a thorough Exploratory Data Analysis of the dataset, work on feature engineering, try out multiple machine learning models, select a model, work to optimize the model, and finally, inspect the outputs of the model and draw conclusions. __While this notebook may not get us to the top of the leaderboard, it is meant to be used as a teaching tool to give you a solid foundation to build on for future machine learning projects. Kaggle projects can teach us a lot about machine learning, but several of the strategies used to get to the very top of the leaderboard are not best practices, so here we'll stick to building a very good - although not quite first place - machine learning solution. While Kaggle projects are competitions, I think they are best described as \"a machine learning education\" disguised as a contest!\"\n",
    "\n",
    "If you are looking to follow-up on this work, I have additional work including a [kernel on using Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/featuretools-for-good) with [Featuretools](https://docs.featuretools.com/#minute-quick-start) for this problem (with slightly higher leaderboard score). (If you enjoy my writing style and explanations, I write for [Towards Data Science](http://medium.com/@williamkoehrsen/))\n",
    "\n",
    "## Problem and Data Explanation\n",
    "\n",
    "The data for this competition is provided in two files: `train.csv` and `test.csv`. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents __one individual__ and each column is a __feature, either unique to the individual, or for the household of the individual__. The training set has one additional column, `Target`, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty. \n",
    "\n",
    "This is a __supervised multi-class classification machine learning problem__:\n",
    "\n",
    "* __Supervised__: provided with the labels for the training data\n",
    "* __Multi-class classification__: Labels are discrete values with 4 classes\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective is to predict poverty on a __household level__. We are given data on the individual level with each individual having unique features but also information about their household. In order to create a dataset for the task, we'll have to perform some _aggregations of the individual data_ for each household. Moreover, we have to make a prediction for every individual in the test set, but _\"ONLY the heads of household are used in scoring\"_ which means we want to predict poverty on a household basis. \n",
    "\n",
    "__Important note: while all members of a household should have the same label in the training data, there are errors where individuals in the same household have different labels. In these cases, we are told to use the label for the head of each household, which can be identified by the rows where `parentesco1 == 1.0`.__ We will cover how to correct this in the notebook (for more info take a look at the [competition main discussion](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403)).\n",
    "\n",
    "The `Target` values represent poverty levels as follows:\n",
    "\n",
    "    1 = extreme poverty \n",
    "    2 = moderate poverty \n",
    "    3 = vulnerable households \n",
    "    4 = non vulnerable households\n",
    "\n",
    "The explanations for all 143 columns can be found in the [competition documentation](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data), but a few to note are below:\n",
    "\n",
    "* __Id__: a unique identifier for each individual, this should not be a feature that we use! \n",
    "* __idhogar__: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier.\n",
    "* __parentesco1__: indicates if this person is the head of the household.\n",
    "* __Target__: the label, which should be equal for all members in a household\n",
    "\n",
    "When we make a model, we'll train on a household basis with the label for each household _the poverty level of the head of household_. The raw data contains a mix of both household and individual characteristics and for the individual data, we will have to find a way to aggregate this for each household. Some of the individuals belong to a household with _no head of household_ which means that unfortunately we can't use this data for training. These issues with the data are completely typical of __real-world__ data and hence this problem is great preparation for the datasets you'll encounter in a data science job! \n",
    "\n",
    "### Metric\n",
    "\n",
    "Ultimately we want to build a machine learning model that can predict the integer poverty level of a household. Our predictions will be assessed by the __Macro F1 Score.__ You may be familiar with the [standard F1 score](http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/) for binary classification problems which is the harmonic mean of precision and recall:\n",
    "\n",
    "$$F_1 = \\frac{2}{\\tfrac{1}{\\mathrm{recall}} + \\tfrac{1}{\\mathrm{precision}}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$$\n",
    "\n",
    "For mutli-class problems, we have to average the F1 scores for each class. The macro F1 score averages the F1 score for each class _without taking into account label imbalances_. \n",
    "\n",
    "$$\\text{Macro F1} = \\frac{\\text{F1 Class 1} + \\text{F1 Class 2} + \\text{F1 Class 3} + \\text{F1 Class 4}}{4}$$\n",
    "\n",
    "In other words, the number of occurrences of each label does not figure into the calculation when using macro (while it does when using the \"weighted\" score). (For more information on the differences, look at the [Scikit-Learn Documention for F1 Score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) or this [Stack Exchange question and answers](https://datascience.stackexchange.com/q/15989/42908). If we want to assess our performance, we can use the code:\n",
    "\n",
    "```\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true, y_predicted, average = 'macro`)\n",
    "```\n",
    "\n",
    "For this problem, the labels are imbalanced, which makes it a little strange to use macro averaging for the evaluation metric, but that's a decision made by the organizers and not something we can change! In your own work, you want to be aware of label imbalances and choose a metric accordingly. \n",
    "\n",
    "## Roadmap\n",
    "\n",
    "The end objective is a machine learning model that can predict the poverty level of a household. However, before we get carried away with modeling, it's important to understand the problem and data. Also, we want to evaluate numerous models before choosing one as the \"best\" and after building a model, we want to investigate the predictions. Our roadmap is therefore as follows:\n",
    "\n",
    "1. Understand the problem (we're almost there already)\n",
    "2. Exploratory Data Analysis\n",
    "3. Feature engineering to create a dataset for machine learning\n",
    "4. Compare several baseline machine learning models\n",
    "5. Try more complex machine learning models\n",
    "6. Optimize the selected model\n",
    "7. Investigate model predictions in context of problem\n",
    "6. Draw conclusions and lay out next steps \n",
    "\n",
    "The steps laid out above are iterative meaning that while we will go through them one at a time, we might go back to an earlier step and revisit some of our decisions. In general, data science is a non-linear pracice where we are constantly evaluating our past decisions and making improvements. In particular, feature engineering, modeling, and optimization are steps that we often repeat because we never know if we got them right the first time!\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "We have a pretty good grasp of the problem, so we'll move into the Exploratory Data Analysis (EDA) and feature engineering. For the EDA we'll examine any interesting anomalies, trends, correlations, or patterns that can be used for feature engineering and for modeling. We'll make sure to investigate our data both quantitatively (with statistics) and visually (with figures). \n",
    "\n",
    "Once we have a good grasp of the data and any potentially useful relationships, we can do some feature engineering (the most important part of the machine learning pipeline) and establish a baseline model. This won't get us to the top of the leaderboard, but it will provide a strong foundation to build on! \n",
    "\n",
    "With all that info in mind (don't worry if you haven't got all the details), let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df5bf2d5d559636504db46b783c45e77d9bb8776"
   },
   "source": [
    "#### Imports\n",
    "\n",
    "We'll use a familiar stack of data science libraries: `Pandas`, `numpy`, `matplotlib`, `seaborn`, and eventually `sklearn` for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a few plotting defaults\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data and Look at Summary Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>v2a1</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>r4h2</th>\n",
       "      <th>r4h3</th>\n",
       "      <th>r4m1</th>\n",
       "      <th>r4m2</th>\n",
       "      <th>r4m3</th>\n",
       "      <th>r4t1</th>\n",
       "      <th>r4t2</th>\n",
       "      <th>r4t3</th>\n",
       "      <th>tamhog</th>\n",
       "      <th>tamviv</th>\n",
       "      <th>escolari</th>\n",
       "      <th>rez_esc</th>\n",
       "      <th>hhsize</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>paredmad</th>\n",
       "      <th>paredzinc</th>\n",
       "      <th>paredfibras</th>\n",
       "      <th>paredother</th>\n",
       "      <th>pisomoscer</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pisoother</th>\n",
       "      <th>pisonatur</th>\n",
       "      <th>pisonotiene</th>\n",
       "      <th>pisomadera</th>\n",
       "      <th>techozinc</th>\n",
       "      <th>techoentrepiso</th>\n",
       "      <th>techocane</th>\n",
       "      <th>techootro</th>\n",
       "      <th>cielorazo</th>\n",
       "      <th>abastaguadentro</th>\n",
       "      <th>abastaguafuera</th>\n",
       "      <th>abastaguano</th>\n",
       "      <th>public</th>\n",
       "      <th>planpri</th>\n",
       "      <th>noelec</th>\n",
       "      <th>coopele</th>\n",
       "      <th>sanitario1</th>\n",
       "      <th>sanitario2</th>\n",
       "      <th>sanitario3</th>\n",
       "      <th>sanitario5</th>\n",
       "      <th>sanitario6</th>\n",
       "      <th>energcocinar1</th>\n",
       "      <th>energcocinar2</th>\n",
       "      <th>energcocinar3</th>\n",
       "      <th>energcocinar4</th>\n",
       "      <th>elimbasu1</th>\n",
       "      <th>elimbasu2</th>\n",
       "      <th>elimbasu3</th>\n",
       "      <th>elimbasu4</th>\n",
       "      <th>elimbasu5</th>\n",
       "      <th>elimbasu6</th>\n",
       "      <th>epared1</th>\n",
       "      <th>epared2</th>\n",
       "      <th>epared3</th>\n",
       "      <th>etecho1</th>\n",
       "      <th>etecho2</th>\n",
       "      <th>etecho3</th>\n",
       "      <th>eviv1</th>\n",
       "      <th>eviv2</th>\n",
       "      <th>eviv3</th>\n",
       "      <th>dis</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>estadocivil1</th>\n",
       "      <th>estadocivil2</th>\n",
       "      <th>estadocivil3</th>\n",
       "      <th>estadocivil4</th>\n",
       "      <th>estadocivil5</th>\n",
       "      <th>estadocivil6</th>\n",
       "      <th>estadocivil7</th>\n",
       "      <th>parentesco1</th>\n",
       "      <th>parentesco2</th>\n",
       "      <th>parentesco3</th>\n",
       "      <th>parentesco4</th>\n",
       "      <th>parentesco5</th>\n",
       "      <th>parentesco6</th>\n",
       "      <th>parentesco7</th>\n",
       "      <th>parentesco8</th>\n",
       "      <th>parentesco9</th>\n",
       "      <th>parentesco10</th>\n",
       "      <th>parentesco11</th>\n",
       "      <th>parentesco12</th>\n",
       "      <th>idhogar</th>\n",
       "      <th>hogar_nin</th>\n",
       "      <th>hogar_adul</th>\n",
       "      <th>hogar_mayor</th>\n",
       "      <th>hogar_total</th>\n",
       "      <th>dependency</th>\n",
       "      <th>edjefe</th>\n",
       "      <th>edjefa</th>\n",
       "      <th>meaneduc</th>\n",
       "      <th>instlevel1</th>\n",
       "      <th>instlevel2</th>\n",
       "      <th>instlevel3</th>\n",
       "      <th>instlevel4</th>\n",
       "      <th>instlevel5</th>\n",
       "      <th>instlevel6</th>\n",
       "      <th>instlevel7</th>\n",
       "      <th>instlevel8</th>\n",
       "      <th>instlevel9</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>overcrowding</th>\n",
       "      <th>tipovivi1</th>\n",
       "      <th>tipovivi2</th>\n",
       "      <th>tipovivi3</th>\n",
       "      <th>tipovivi4</th>\n",
       "      <th>tipovivi5</th>\n",
       "      <th>computer</th>\n",
       "      <th>television</th>\n",
       "      <th>mobilephone</th>\n",
       "      <th>qmobilephone</th>\n",
       "      <th>lugar1</th>\n",
       "      <th>lugar2</th>\n",
       "      <th>lugar3</th>\n",
       "      <th>lugar4</th>\n",
       "      <th>lugar5</th>\n",
       "      <th>lugar6</th>\n",
       "      <th>area1</th>\n",
       "      <th>area2</th>\n",
       "      <th>age</th>\n",
       "      <th>SQBescolari</th>\n",
       "      <th>SQBage</th>\n",
       "      <th>SQBhogar_total</th>\n",
       "      <th>SQBedjefe</th>\n",
       "      <th>SQBhogar_nin</th>\n",
       "      <th>SQBovercrowding</th>\n",
       "      <th>SQBdependency</th>\n",
       "      <th>SQBmeaned</th>\n",
       "      <th>agesq</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_279628684</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21eb7fcc1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>10</td>\n",
       "      <td>no</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>100</td>\n",
       "      <td>1849</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1849</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_f29eb3ddd</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0e5d7a658</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>no</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>144</td>\n",
       "      <td>4489</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4489</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_68de51c94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2c7317ea8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>no</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>121</td>\n",
       "      <td>8464</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>8464</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_d671db89c</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2b58d945f</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>11</td>\n",
       "      <td>no</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>81</td>\n",
       "      <td>289</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>289</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_d56d6f5f5</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2b58d945f</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>11</td>\n",
       "      <td>no</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>121</td>\n",
       "      <td>1369</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id      v2a1  hacdor  rooms  hacapo  v14a  refrig  v18q  v18q1  \\\n",
       "0  ID_279628684  190000.0       0      3       0     1       1     0    NaN   \n",
       "1  ID_f29eb3ddd  135000.0       0      4       0     1       1     1    1.0   \n",
       "2  ID_68de51c94       NaN       0      8       0     1       1     0    NaN   \n",
       "3  ID_d671db89c  180000.0       0      5       0     1       1     1    1.0   \n",
       "4  ID_d56d6f5f5  180000.0       0      5       0     1       1     1    1.0   \n",
       "\n",
       "   r4h1  r4h2  r4h3  r4m1  r4m2  r4m3  r4t1  r4t2  r4t3  tamhog  tamviv  \\\n",
       "0     0     1     1     0     0     0     0     1     1       1       1   \n",
       "1     0     1     1     0     0     0     0     1     1       1       1   \n",
       "2     0     0     0     0     1     1     0     1     1       1       1   \n",
       "3     0     2     2     1     1     2     1     3     4       4       4   \n",
       "4     0     2     2     1     1     2     1     3     4       4       4   \n",
       "\n",
       "   escolari  rez_esc  hhsize  paredblolad  paredzocalo  paredpreb  pareddes  \\\n",
       "0        10      NaN       1            1            0          0         0   \n",
       "1        12      NaN       1            0            0          0         0   \n",
       "2        11      NaN       1            0            0          0         0   \n",
       "3         9      1.0       4            1            0          0         0   \n",
       "4        11      NaN       4            1            0          0         0   \n",
       "\n",
       "   paredmad  paredzinc  paredfibras  paredother  pisomoscer  pisocemento  \\\n",
       "0         0          0            0           0           1            0   \n",
       "1         1          0            0           0           0            0   \n",
       "2         1          0            0           0           1            0   \n",
       "3         0          0            0           0           1            0   \n",
       "4         0          0            0           0           1            0   \n",
       "\n",
       "   pisoother  pisonatur  pisonotiene  pisomadera  techozinc  techoentrepiso  \\\n",
       "0          0          0            0           0          0               1   \n",
       "1          0          0            0           1          1               0   \n",
       "2          0          0            0           0          1               0   \n",
       "3          0          0            0           0          1               0   \n",
       "4          0          0            0           0          1               0   \n",
       "\n",
       "   techocane  techootro  cielorazo  abastaguadentro  abastaguafuera  \\\n",
       "0          0          0          1                1               0   \n",
       "1          0          0          1                1               0   \n",
       "2          0          0          1                1               0   \n",
       "3          0          0          1                1               0   \n",
       "4          0          0          1                1               0   \n",
       "\n",
       "   abastaguano  public  planpri  noelec  coopele  sanitario1  sanitario2  \\\n",
       "0            0       1        0       0        0           0           1   \n",
       "1            0       1        0       0        0           0           1   \n",
       "2            0       1        0       0        0           0           1   \n",
       "3            0       1        0       0        0           0           1   \n",
       "4            0       1        0       0        0           0           1   \n",
       "\n",
       "   sanitario3  sanitario5  sanitario6  energcocinar1  energcocinar2  \\\n",
       "0           0           0           0              0              0   \n",
       "1           0           0           0              0              1   \n",
       "2           0           0           0              0              1   \n",
       "3           0           0           0              0              1   \n",
       "4           0           0           0              0              1   \n",
       "\n",
       "   energcocinar3  energcocinar4  elimbasu1  elimbasu2  elimbasu3  elimbasu4  \\\n",
       "0              1              0          1          0          0          0   \n",
       "1              0              0          1          0          0          0   \n",
       "2              0              0          1          0          0          0   \n",
       "3              0              0          1          0          0          0   \n",
       "4              0              0          1          0          0          0   \n",
       "\n",
       "   elimbasu5  elimbasu6  epared1  epared2  epared3  etecho1  etecho2  etecho3  \\\n",
       "0          0          0        0        1        0        1        0        0   \n",
       "1          0          0        0        1        0        0        1        0   \n",
       "2          0          0        0        1        0        0        0        1   \n",
       "3          0          0        0        0        1        0        0        1   \n",
       "4          0          0        0        0        1        0        0        1   \n",
       "\n",
       "   eviv1  eviv2  eviv3  dis  male  female  estadocivil1  estadocivil2  \\\n",
       "0      1      0      0    0     1       0             0             0   \n",
       "1      0      1      0    0     1       0             0             0   \n",
       "2      0      0      1    1     0       1             0             0   \n",
       "3      0      0      1    0     1       0             0             0   \n",
       "4      0      0      1    0     0       1             0             1   \n",
       "\n",
       "   estadocivil3  estadocivil4  estadocivil5  estadocivil6  estadocivil7  \\\n",
       "0             0             1             0             0             0   \n",
       "1             0             1             0             0             0   \n",
       "2             0             0             0             1             0   \n",
       "3             0             0             0             0             1   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   parentesco1  parentesco2  parentesco3  parentesco4  parentesco5  \\\n",
       "0            1            0            0            0            0   \n",
       "1            1            0            0            0            0   \n",
       "2            1            0            0            0            0   \n",
       "3            0            0            1            0            0   \n",
       "4            0            1            0            0            0   \n",
       "\n",
       "   parentesco6  parentesco7  parentesco8  parentesco9  parentesco10  \\\n",
       "0            0            0            0            0             0   \n",
       "1            0            0            0            0             0   \n",
       "2            0            0            0            0             0   \n",
       "3            0            0            0            0             0   \n",
       "4            0            0            0            0             0   \n",
       "\n",
       "   parentesco11  parentesco12    idhogar  hogar_nin  hogar_adul  hogar_mayor  \\\n",
       "0             0             0  21eb7fcc1          0           1            0   \n",
       "1             0             0  0e5d7a658          0           1            1   \n",
       "2             0             0  2c7317ea8          0           1            1   \n",
       "3             0             0  2b58d945f          2           2            0   \n",
       "4             0             0  2b58d945f          2           2            0   \n",
       "\n",
       "   hogar_total dependency edjefe edjefa  meaneduc  instlevel1  instlevel2  \\\n",
       "0            1         no     10     no      10.0           0           0   \n",
       "1            1          8     12     no      12.0           0           0   \n",
       "2            1          8     no     11      11.0           0           0   \n",
       "3            4        yes     11     no      11.0           0           0   \n",
       "4            4        yes     11     no      11.0           0           0   \n",
       "\n",
       "   instlevel3  instlevel4  instlevel5  instlevel6  instlevel7  instlevel8  \\\n",
       "0           0           1           0           0           0           0   \n",
       "1           0           0           0           0           0           1   \n",
       "2           0           0           1           0           0           0   \n",
       "3           0           1           0           0           0           0   \n",
       "4           0           0           1           0           0           0   \n",
       "\n",
       "   instlevel9  bedrooms  overcrowding  tipovivi1  tipovivi2  tipovivi3  \\\n",
       "0           0         1      1.000000          0          0          1   \n",
       "1           0         1      1.000000          0          0          1   \n",
       "2           0         2      0.500000          1          0          0   \n",
       "3           0         3      1.333333          0          0          1   \n",
       "4           0         3      1.333333          0          0          1   \n",
       "\n",
       "   tipovivi4  tipovivi5  computer  television  mobilephone  qmobilephone  \\\n",
       "0          0          0         0           0            1             1   \n",
       "1          0          0         0           0            1             1   \n",
       "2          0          0         0           0            0             0   \n",
       "3          0          0         0           0            1             3   \n",
       "4          0          0         0           0            1             3   \n",
       "\n",
       "   lugar1  lugar2  lugar3  lugar4  lugar5  lugar6  area1  area2  age  \\\n",
       "0       1       0       0       0       0       0      1      0   43   \n",
       "1       1       0       0       0       0       0      1      0   67   \n",
       "2       1       0       0       0       0       0      1      0   92   \n",
       "3       1       0       0       0       0       0      1      0   17   \n",
       "4       1       0       0       0       0       0      1      0   37   \n",
       "\n",
       "   SQBescolari  SQBage  SQBhogar_total  SQBedjefe  SQBhogar_nin  \\\n",
       "0          100    1849               1        100             0   \n",
       "1          144    4489               1        144             0   \n",
       "2          121    8464               1          0             0   \n",
       "3           81     289              16        121             4   \n",
       "4          121    1369              16        121             4   \n",
       "\n",
       "   SQBovercrowding  SQBdependency  SQBmeaned  agesq  Target  \n",
       "0         1.000000            0.0      100.0   1849       4  \n",
       "1         1.000000           64.0      144.0   4489       4  \n",
       "2         0.250000           64.0      121.0   8464       4  \n",
       "3         1.777778            1.0      121.0    289       4  \n",
       "4         1.777778            1.0      121.0   1369       4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 150\n",
    "\n",
    "# Read in data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ae7f4f9dc01ad847283246327c7db467ba76498"
   },
   "source": [
    "That gives us a look at all of the columns which don't appear to be in any order. To get a quick overview of the data we use `df.info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b180722a8e9a1688463f0340e4830e37a5586c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9557 entries, 0 to 9556\n",
      "Columns: 143 entries, Id to Target\n",
      "dtypes: float64(8), int64(130), object(5)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35a4c874b82d853208f32d0d1dbaed422045f914"
   },
   "source": [
    "This tells us there are 130 integer columns, 8 float (numeric) columns, and 5 object columns. The integer columns probably represent Boolean variables (that take on either 0 or 1) or [ordinal variables](https://www.ma.utexas.edu/users/mks/statmistakes/ordinal.html) with discrete ordered values. The object columns might pose an issue because they cannot be fed directly into a machine learning model.\n",
    "\n",
    "Let's glance at the test data which has many more rows (individuals) than the train. It does have one fewer column because there's no Target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "61687f25325218b77d4d75804044c7083da85eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23856 entries, 0 to 23855\n",
      "Columns: 142 entries, Id to agesq\n",
      "dtypes: float64(8), int64(129), object(5)\n",
      "memory usage: 25.8+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c0d0640f3867d2c290ac58a55639fefb08875c0"
   },
   "source": [
    "#### Integer Columns\n",
    "\n",
    "Let's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7db394756abedd215885bc7d6b9dc2c767964b35"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28ce83e56d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', figsize = (8, 6));\n",
    "plt.xlabel('Number of Unique Values'); plt.ylabel('Count');\n",
    "plt.title('Count of Unique Values in Integer Columns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a9db9a6caa3b77b6a6f0184644c1e11b2ed53d5"
   },
   "source": [
    "The columns with only 2 unique values represent Booleans (0 or 1). In a lot of cases, this boolean information is already on a household level. For example, the `refrig` column says whether or not the household has a refrigerator. When it comes time to make features from the Boolean columns that are on the household level, we will _not need to aggregate_ these. However, the Boolean columns that are on the individual level will need to be aggregated. \n",
    "\n",
    "#### Float Columns\n",
    "\n",
    "Another column type is floats which represent continuous variables. We can make a quick distribution plot to show the distribution of all float columns. We'll use an [`OrderedDict`](https://pymotw.com/2/collections/ordereddict.html) to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).\n",
    "\n",
    "The following graphs shows the distributions of the `float` columns colored by the value of the `Target`. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c22a1876cf1d8a717b0baab8564acb4a7f531b51",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "plt.figure(figsize = (20, 16))\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Color mapping\n",
    "colors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\n",
    "poverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n",
    "\n",
    "# Iterate through the float columns\n",
    "for i, col in enumerate(train.select_dtypes('float')):\n",
    "    ax = plt.subplot(4, 2, i + 1)\n",
    "    # Iterate through the poverty levels\n",
    "    for poverty_level, color in colors.items():\n",
    "        # Plot each poverty level as a separate line\n",
    "        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n",
    "                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n",
    "        \n",
    "    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n",
    "\n",
    "plt.subplots_adjust(top = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79ec245f541336c564fdce3d013b0b988627b6e5"
   },
   "source": [
    "Later on we'll calculate correlations between the variables and the `Target` to gauge the relationships between the features, but these plots can already give us a sense of which variables may be most \"relevant\" to a model. For example, the `meaneduc`, representing the average education of the adults in the household appears to be related to the poverty level: __a higher average adult education leads to higher values of the target which are less severe levels of poverty__. The theme of the importance of education is one we will come back to again and again in this notebook! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "355fd9746ec30af6661cf97a1243790730a5df99"
   },
   "source": [
    "#### Object Columns\n",
    "\n",
    "The last column type is `object` which we can view as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b110b76084ba14045c4c69ce909148d91ce0ecc"
   },
   "outputs": [],
   "source": [
    "train.select_dtypes('object').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1e660b7e06b81b61bec81a82be4a5854115d09a"
   },
   "source": [
    "The `Id` and `idhogar` object types make sense because these are identifying variables. However, the other columns seem to be a mix of strings and numbers which we'll need to address before doing any machine learning. According to the documentation for these columns:\n",
    "\n",
    "* `dependency`: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n",
    "* `edjefe`: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n",
    "* `edjefa`: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n",
    "\n",
    "These explanations clear up the issue. For these three variables, __\"yes\" = 1__ and __\"no\" = 0__. We can correct the variables using a mapping and convert to floats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"yes\": 1, \"no\": 0}\n",
    "\n",
    "# Apply same operation to both train and test\n",
    "for df in [train, test]:\n",
    "    # Fill in the values with the correct mapping\n",
    "    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n",
    "    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n",
    "    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n",
    "\n",
    "train[['dependency', 'edjefa', 'edjefe']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are now correctly represented as numbers and can be fed into a machine learning model. \n",
    "\n",
    "To make operations like that above a little easier, we'll join together the training and testing dataframes. This is important once we start feature engineering because we want to apply the same operations to both dataframes so we end up with the same features. Later we can separate out the sets based on the `Target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add null Target column to test\n",
    "test['Target'] = np.nan\n",
    "data = train.append(test, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Label Distribution\n",
    "\n",
    "Next, we can get an idea of how imbalanced the problem is by looking at the distribution of labels. There are four possible integer levels, indicating four different levels of poverty. To look at the correct labels, we'll subset only to the columns where `parentesco1 == 1` because this is the head of household, the correct label for each household.\n",
    "\n",
    "The bar plot below shows the distribution of training labels (since there are no testing labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heads of household\n",
    "heads = data.loc[data['parentesco1'] == 1].copy()\n",
    "\n",
    "# Labels for training\n",
    "train_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1), ['Target', 'idhogar']]\n",
    "\n",
    "# Value counts of target\n",
    "label_counts = train_labels['Target'].value_counts().sort_index()\n",
    "\n",
    "# Bar plot of occurrences of each label\n",
    "label_counts.plot.bar(figsize = (6, 4), \n",
    "                      color = colors.values())\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('Poverty Level'); plt.ylabel('Count'); \n",
    "plt.xticks([x - 1 for x in poverty_mapping.keys()], list(poverty_mapping.values()))\n",
    "plt.title('Poverty Level Breakdown');\n",
    "\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with an imbalanced class problem (which makes it intriguing why the contest organizers choose the _macro_ F1 score as the metric instead of _weighted_ F1!). There are many more households that classify as _non vulnerable_ than in any other category. The _extreme_ poverty class is the smallest (I guess this should make us optimistic!).\n",
    "\n",
    "One problem with imbalanced classification problems is that the machine learning model can have a difficult time predicting the minority classes because it sees far less examples. Think about this in human terms: if we are classifiying poverty and we see far more cases of no poverty than extreme poverty, it will make it more difficult for us to identify the high poverty households because of less exposure. One potential method to address class imbalanceds is through oversampling  (which is covered in more advanced notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Wrong Labels\n",
    "\n",
    "As with any realistic dataset, the Costa Rican Poverty data has some issues. Typically, 80% of a data science project will be spent cleaning data and fixing anomalies/errors. These can be either human entry errors, measurement errors, or sometimes just extreme values that are correct but stand out. For this problem, some of the labels are not correct because _individuals in the same household have a different poverty level_. We're not told why this may be the case, but we are told to use the head of household as the true label. \n",
    "\n",
    "That information makes our job much easier, but in a real-world problem, we would have to figure out the reason _Why_ the labels are wrong and how to address the issue on our own. This section fixes the issue with the labels although it is not strictly necessary: I kept it in the notebook just to show how we may deal with this issue.\n",
    "\n",
    "### Identify Errors\n",
    "\n",
    "First we need to find the errors before we can correct them. To find the households with different labels for  family members, we can group the data by the household and then check if there is only one unique value of the `Target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The organizers tell us that the correct label is that for the head of household, where `parentesco1 == 1`. For this household, the correct label is __3__ for all members. We can correct this (as shown later) by reassigning all the individuals in this household the correct poverty level. In the real-world, you might have to make the tough decision of how to address the problem by yourself (or with the help of your team)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Families without Heads of Household\n",
    "\n",
    "We can correct all the label discrepancies by assigning the individuals in the same household the label of the head of household. But wait, you may ask: \"What if there are households without a head of household? And what if the members of those households have differing values of the label?\" \n",
    "\n",
    "Well, since you asked, let's investigate exactly that question! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "households_leader = train.groupby('idhogar')['parentesco1'].sum()\n",
    "\n",
    "# Find households without a head\n",
    "households_no_head = train.loc[train['idhogar'].isin(households_leader[households_leader == 0].index), :]\n",
    "\n",
    "print('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find households without a head and where labels are different\n",
    "households_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "print('{} Households with no head have different labels.'.format(sum(households_no_head_equal == False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's a relief! This means that we don't have to worry about a household both where there is no head __AND__ the members have different values of the label! For this problem, according to the organizers, __if a household does not have a head, then there is no true label. Therefore, we actually won't use any of the households without a head for training__ Nonetheless, it's still a good exercise to go through this process of investigating the data! \n",
    "\n",
    "### Correct Errors\n",
    "\n",
    "Now we can correct labels for the households that do have a head __AND__ the members have different poverty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each household\n",
    "for household in not_equal.index:\n",
    "    # Find the correct label (for the head of household)\n",
    "    true_target = int(train[(train['idhogar'] == household) & (train['parentesco1'] == 1.0)]['Target'])\n",
    "    \n",
    "    # Set the correct label for all members in the household\n",
    "    train.loc[train['idhogar'] == household, 'Target'] = true_target\n",
    "    \n",
    "    \n",
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only going to use the heads of household for the labels, __this step is not completely necessary but it shows a workflow for correcting data errors like you may encounter in real life__. Don't consider it extra work, just practice for your career! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Variables\n",
    "\n",
    "One of the most important steps of exploratory data analysis is finding missing values in the data and determining how to handle them. Missing values have to be filled in before we use a machine learning model and we need to think of the best strategy for filling them in based on the feature: this is where we'll have to start digging into the data definitions.\n",
    "\n",
    "First we can look at the percentage of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing in each column\n",
    "missing = pd.DataFrame(data.isnull().sum()).rename(columns = {0: 'total'})\n",
    "\n",
    "# Create a percentage missing\n",
    "missing['percent'] = missing['total'] / len(data)\n",
    "\n",
    "missing.sort_values('percent', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to worry about the `Target` becuase we made that `NaN` for the test data. However, we do need to address the other 3 columns with a high percentage of missing values.\n",
    "\n",
    "__v18q1__: Number of tablets\n",
    "\n",
    "Let's start with `v18q1` which indicates the number of tablets owned by a family. We can look at the value counts of this variable. Since this is a household variable, it only makes sense to look at it on a household level, so we'll only select the rows for the head of household.\n",
    "\n",
    "#### Function to Plot Value Counts\n",
    "\n",
    "Since we might want to plot value counts for different columns, we can write a simple function that will do it for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_counts(df, col, heads_only = False):\n",
    "    \"\"\"Plot value counts of a column, optionally with only the heads of a household\"\"\"\n",
    "    # Select heads of household\n",
    "    if heads_only:\n",
    "        df = df.loc[df['parentesco1'] == 1].copy()\n",
    "        \n",
    "    plt.figure(figsize = (8, 6))\n",
    "    df[col].value_counts().sort_index().plot.bar(color = 'blue',\n",
    "                                                 edgecolor = 'k',\n",
    "                                                 linewidth = 2)\n",
    "    plt.xlabel(f'{col}'); plt.title(f'{col} Value Counts'); plt.ylabel('Count')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_counts(heads, 'v18q1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the most common number of tablets to own is 1 if we go only by the data that is present. However, we also need to think about the data that is missing. In this case, it could be that families with a `nan` in this category just do not own a tablet! If we look at the data definitions, we see that `v181` indicates whether or not a family owns a tablet. We should investigate this column combined with the number of tablets to see if our hypothesis holds.\n",
    "\n",
    "We can `groupby` the value of `v18q` (which is 1 for owns a tablet and 0 for does not) and then calculate the number of null values for `v18q1`. This will tell us if the null values represent that the family does not own a tablet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that solves the issue! Every family that has `nan` for `v18q1` does not own a tablet. Therefore, we can fill in this missing value with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['v18q1'] = data['v18q1'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__v2a1__: Monthly rent payment\n",
    "\n",
    "The next missing column is `v2a1` which represents the montly rent payment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can look at the distribution of this value by the `Target`. This shows only the households that have a measurement of this variable.\n",
    "sns.violinplot(x = 'Target', y = 'v2a1', data = data, figsize = (10, 6));\n",
    "It appears there is at least one outlier in this value with a target of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to looking at the missing values of the monthly rent payment, it will be interesting to also look at the distribution of `tipovivi_`, the columns showing the ownership/renting status of the home. For this plot, we show the ownership status of those homes that do not have a monthly rent payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables indicating home ownership\n",
    "own_variables = [x for x in data if x.startswith('tipo')]\n",
    "\n",
    "\n",
    "# Plot of the home ownership variables for home missing rent payments\n",
    "data.loc[data['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (8, 6),\n",
    "                                                                        color = 'purple');\n",
    "\n",
    "plt.title('Home Ownership Status for datas Missing Rent Payments', size = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of the home ownership variables is below:\n",
    "\n",
    "    tipovivi1, =1 own and fully paid house\n",
    "    tipovivi2, \"=1 own,  paying in installments\"\n",
    "    tipovivi3, =1 rented\n",
    "    tipovivi4, =1 precarious\n",
    "    tipovivi5, \"=1 other(assigned,  borrowed)\"\n",
    "    \n",
    "We've solved the issue! Well, mostly: the households that do not have a monthly rent payment generally own their own home. In a few other situations, we are not sure of the reason for the missing information. \n",
    "\n",
    "For the houses that are owned and have a missing monthly rent payment, we can set the value of the rent payment to 0. For the other homes, we can leave the missing values to be imputed but we'll add a flag (Boolean) column indicating that these households had missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in households that own the house with 0 rent payment\n",
    "data.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n",
    "\n",
    "# Create missing rent payment column\n",
    "data['v2a1-missing'] = data['v2a1'].isnull()\n",
    "\n",
    "data['v2a1-missing'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__rez_esc__: years behind in school\n",
    "\n",
    "The last column with a high percentage of missing values is `rez_esc` indicating years behind in school. For the families with a null value, is possible that they have no children currently in school. Let's test this out by finding the ages of those who have a missing value in this column and the ages of those who do not have a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['rez_esc'].notnull()]['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this tells us is that the oldest age with a missing value is 17. For anyone older than this, maybe we can assume that they are simply not in school. Let's look at the ages of those who have a missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['rez_esc'].isnull()]['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The people who are over 17 are presumably out of school and therefore cannot be behind in school. For this value, if the individual is over 17 and they have a missing value, we can set it to zero. For anyone else, we'll leave the value to be imputed and add a boolean flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If individual is over 17 and missing years behind, set it to 0\n",
    "data.loc[(data['age'] > 17) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n",
    "\n",
    "# Add a flag for those 17 and younger with a missing value\n",
    "data['rez_esc-missing'] = data['rez_esc'].isnull()\n",
    "\n",
    "data['rez_esc-missing'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining missing values in each column will be filled in, a process known as `Imputation`. We can select the type of imputation to use, and one of the simplest and most effective methods is to fill in the missing values with the `median` of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step with the missing values, we can plot the distribution of target for the case where either of these values are missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_counts(data[(data['rez_esc-missing'] == 1)], 'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution here seems to match that for all the data at large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_counts(data[(data['v2a1-missing'] == 1)], 'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like it could be an indicator of more poverty given the higher prevalence of 2: moderate poverty. \n",
    "\n",
    "__This represents an important point__: sometimes the missing information is just as important as the information you are given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "There is plenty more exploratory data analysis we can do, but first we should work on consolidating our data at a household level. We already have some of the information for each household, but for training, we will need _all_ of the information summarized for each household. This means grouping the individuals in a house (`groupby`) and performing an aggregation (`agg`) of the individual variables. \n",
    "\n",
    "In another notebook, I show how we can use automated feature engineering to do this, and __automated feature engineering__ should be a standard part of the machine learning workflow. Right now, we'll stick to doing this by hand, but definitely take a look at __automated feature engineering in Featuretools__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Definitions\n",
    "\n",
    "Sometimes in data science we have to get our hands dirty digging through the data or do tedious tasks that take a lot of time. This is that part of the analysis: we have to define the columns that are at an individual level and at a household level. This is where we have to get our hands dirty by digging through the [data decsriptions](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data). There is simply no other way to identify which variables at are the household level than to go through the variables themselves in the data description. Except, I've already done this for you, so all you have to do is copy and paste (after _checking my work_ of course). \n",
    "\n",
    "We'll define different variables because we need to treat some of them in a different manner. Once we have the variables defined on each level, we can work to start aggregating them as needed.\n",
    "\n",
    "The process is as follows\n",
    "\n",
    "1. Break variables into household level and invididual level\n",
    "2. Find suitable aggregations for the individual level data\n",
    "    * Ordinal variables can use statistical aggregations\n",
    "    * Boolean variables can also be aggregated but with fewer stats\n",
    "3. Join the individual aggregations to the household level data\n",
    "\n",
    "### Define Variable Categories\n",
    "\n",
    "There are several different categories of variables:\n",
    "\n",
    "1. Individual Variables: these are characteristics of each individual rather than the household\n",
    "    * Boolean: Yes or No (0 or 1)\n",
    "    * Ordered Discrete: Integers with an ordering\n",
    "2. Household variables\n",
    "    * Boolean: Yes or No\n",
    "    * Ordered Discrete: Integers with an ordering\n",
    "    * Continuous numeric\n",
    "3. Squared Variables: derived from squaring variables in the data\n",
    "4. Id variables: identifies the data and should not be used as features\n",
    "\n",
    "Below we manually define the variables in each category. This is a little tedious, but also necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = ['Id', 'idhogar', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc6f4e6c5f343e917d65a1d8e6c1043236bcdce1"
   },
   "outputs": [],
   "source": [
    "ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n",
    "            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n",
    "            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n",
    "            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n",
    "            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n",
    "            'instlevel9', 'mobilephone', 'rez_esc-missing']\n",
    "\n",
    "ind_ordered = ['rez_esc', 'escolari', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b382ddbea1cd6ff25b71f13b9263a6565ebadd57"
   },
   "outputs": [],
   "source": [
    "hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n",
    "           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n",
    "           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n",
    "           'pisonatur', 'pisonotiene', 'pisomadera',\n",
    "           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n",
    "           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n",
    "            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n",
    "           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n",
    "           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n",
    "           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n",
    "           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n",
    "           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n",
    "           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n",
    "           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n",
    "           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n",
    "\n",
    "hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n",
    "              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n",
    "              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n",
    "\n",
    "hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2771c641c24bb3ed21eb655320a4ad85066a2cd6"
   },
   "outputs": [],
   "source": [
    "sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n",
    "        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we covered all of the variables and didn't repeat any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))\n",
    "print('We covered every variable: ', len(x) == data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39d839a711f0783285ff0ca570366f045eb995f1"
   },
   "source": [
    "#### Squared Variables\n",
    "\n",
    "First, the easiest step: we'll remove all of the squared variables. Sometimes variables are squared or transformed as part of feature engineering because it can help linear models learn relationships that are non-linear. However, since we will be using more complex models, these squared features are redundant. They are highly correlated with the non-squared version, and hence can actually hurt our model by adding irrelevant information and also slowing down training.\n",
    "\n",
    "For an example, let's take a look at `SQBage` vs `age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('age', 'SQBage', data = data, fit_reg=False);\n",
    "plt.title('Squared Age versus Age');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are highly correlated, and hence I don't think we need both in our data. However, I'll keep a copy of the squared variables so we can experiment with whether or not these help the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_data = data[sqr_].copy()\n",
    "\n",
    "# Remove squared variables\n",
    "data = data.drop(columns = sqr_)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Id Variables\n",
    "\n",
    "These are pretty simple: they will be kept as is.\n",
    "\n",
    "## Household Level Variables\n",
    "\n",
    "These will mostly be kept as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = data.loc[data['parentesco1'] == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = heads[id_ + hh_bool + hh_cont + hh_ordered]\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundant Household Variables\n",
    "\n",
    "Let's take a look at the correlations between all of the household variables. If there are any that are too highly correlated, then we might want to remove one of the pair of highly correlated variables.\n",
    "\n",
    "The following code identifies any variables with a greater than 0.9 absolute magnitude correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = heads.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n",
    "\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are one out of each pair of correlated variables. To find the other pair, we can subset the `corr_matrix`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several variables here having to do with the size of the house:\n",
    "\n",
    "* r4t3, Total persons in the household\n",
    "* tamhog, size of the household\n",
    "* tamviv, number of persons living in the household\n",
    "* hhsize, household size\n",
    "* hogar_total, # of total individuals in the household\n",
    "\n",
    "These variables are all highly correlated with one another. In fact, `hhsize` has a perfect correlation with `tamhog` and `hogar_total`. __We will remove these two variables because the information is redundant.__ We can also remove `r4t3` because it has a near perfect correlation with `hhsize`.\n",
    "\n",
    "`tamviv` is not necessarily the same as `hhsize` because there might be family members that are not living in the household. We can leave in both of these columns and let's visualize this relationship in a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('tamviv', 'hhsize', data, fit_reg=False, size = 8);\n",
    "plt.title('Household size vs number of persons living in the household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for a number of cases, there are more people living in the household than there are in the family. This gives us a good idea for a new feature: __the difference between these two measurements!__\n",
    "\n",
    "Let's make this new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads['hhsize-diff'] = heads['tamviv'] - heads['hhsize']\n",
    "plot_value_counts(heads, 'hhsize-diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though most households do not have a difference, there are a few that have more people living in the household than are members of the household.\n",
    "\n",
    "Let's move on to the other redundant variables. First we can look at `coopele`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.loc[corr_matrix['coopele'].abs() > 0.9, corr_matrix['coopele'].abs() > 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables indicate where the electricity in the home is coming from. There are four options, and the families that don't have one of these two options either have no electricity or get it from a private plant. \n",
    "\n",
    "I'm going to compress these four variables into one by creating an ordinal variable. I'm going to choose the mapping myself, based on the data decriptions: \n",
    "\n",
    "0: No electricity\n",
    "1: Electricity from cooperative\n",
    "2: Electricity from CNFL, ICA, ESPH/JASEC\n",
    "3: Electricity from private plant\n",
    "\n",
    "An ordered variable has an inherent ordering, and for this we choose our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = []\n",
    "\n",
    "# Assign values\n",
    "for i, row in heads.iterrows():\n",
    "    if row['noelec'] == 1:\n",
    "        elec.append(0)\n",
    "    elif row['coopele'] == 1:\n",
    "        elec.append(1)\n",
    "    elif row['public'] == 1:\n",
    "        elec.append(2)\n",
    "    elif row['planpri'] == 1:\n",
    "        elec.append(3)\n",
    "    else:\n",
    "        elec.append(np.nan)\n",
    "        \n",
    "# Record the new variable and missing flag\n",
    "heads['elec'] = elec\n",
    "heads['elec-missing'] = heads['elec'].isnull()\n",
    "\n",
    "# Remove the electricity columns\n",
    "heads = heads.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_counts(heads[heads['elec-missing'] == True], 'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final redundant column is `area2`. This means the house is in a rural zone, but it's redundant because we have a column indicating if the house is in a urban zone. Therefore, we can drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = heads.drop(columns = 'area2')\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads['walls'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]),\n",
    "                           axis = 1)\n",
    "heads['walls'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),\n",
    "                           axis = 1)\n",
    "heads['roof'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),\n",
    "                           axis = 1)\n",
    "heads['floor'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']\n",
    "heads['walls+roof+floor'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads.groupby(['walls+roof+floor'])['Target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No toilet, no electricity, no floor, no water service, no ceiling\n",
    "heads['warning'] = -1 * (heads['sanitario1'] + \n",
    "                         (heads['elec'] == 0) + \n",
    "                         heads['pisonotiene'] + \n",
    "                         heads['abastaguano']) + -1 * (heads['cielorazo'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x = 'warning', y = 'Target', data = heads,\n",
    "              figsize = (10, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads['bonus'] = 1 * (heads['v14a'] + \n",
    "                      heads['refrig'] + \n",
    "                      heads['computer'] + \n",
    "                      heads['cielorazo'] + \n",
    "                      heads['television'] + \n",
    "                      heads['tipovivi1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot('bonus', 'Target', data = heads,\n",
    "                figsize = (10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Household Level Variables\n",
    "\n",
    "After going to all that trouble of figuring out these variables, now we can take a look at these in plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Correlations\n",
    "\n",
    "There are many ways for measuring correlations between two variables. Here we will examine two of these:\n",
    "\n",
    "1. The Pearson Correlation: from -1 to 1 measuring the linear relationship between two variables\n",
    "2. The Spearman Correlation: from -1 to 1 measuring the monotic relationship between two variables\n",
    "\n",
    "These are best illustrated by example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr([1, 2, 3], [4, 5, 6]).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corrs(x, y):\n",
    "    spr = spearmanr(x, y).correlation\n",
    "    pcr = np.corrcoef(x, y)[0, 1]\n",
    "    \n",
    "    data = pd.DataFrame({'x': x, 'y': y})\n",
    "    plt.figure( figsize = (6, 4))\n",
    "    p = sns.regplot('x', 'y', data = data, fit_reg = False);\n",
    "    plt.title(f'Spearman: {round(spr, 2)}; Pearson: {round(pcr, 2)}');\n",
    "    p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(100))\n",
    "y = x ** 2\n",
    "\n",
    "plot_corrs(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(-99, 100))\n",
    "y = x ** 2\n",
    "\n",
    "plot_corrs(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(-9, 10))\n",
    "y = 2 * np.sin(x)\n",
    "\n",
    "plot_corrs(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(50))\n",
    "y = 1.01 * x\n",
    "plot_corrs(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Level Variables\n",
    "\n",
    "There are two types of individual level variables: Boolean (1 or 0 for True or False) and Continuous (can take on any value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = data[id_ + ind_bool + ind_ordered]\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redundant Individual Variables\n",
    "\n",
    "We can do the same process we did with the household level variables to identify any redundant individual variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = ind.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.90)]\n",
    "\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply the opposite of male! We can remove the male flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ind.drop(columns = 'male')\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[[c for c in ind if c.startswith('instl')]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)\n",
    "ind['inst'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x = 'Target', y = 'inst', data = ind);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ind.drop(columns = [c for c in ind if c.startswith('instlevel')])\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std'])\n",
    "ind_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = []\n",
    "for c in ind_agg.columns.levels[0]:\n",
    "    for stat in ind_agg.columns.levels[1]:\n",
    "        new_col.append(f'{c}-{stat}')\n",
    "        \n",
    "ind_agg.columns = new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = heads.corr()['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs.sort_values().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs.sort_values().dropna().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Variables\n",
    "\n",
    "* We'll start with the easiest: the __id variables__ should simply be kept in the data because the `idhogar` is how we can group together members of the same household. \n",
    "* The __squared variables will be removed__. With a gradient boosting machine, squaring the variables will not provide any benefits and can even hurt the model because the correlation between the squared variable and the original variable is high (these are called [__collinear variables__](https://en.wikipedia.org/wiki/Multicollinearity)). It's possible this transform could be useful for other models, but for now we can remove these columns. \n",
    "* The household level features will be left as is.  There is only one household level feature for each household (well hopefully unless the data is off) so we can already use these for training a model\n",
    "\n",
    "Let's implement these three steps and look at the data we have. __We will build all our features on the `household` dataframe becuase we want to make one row for each household.__ We will join the training data to the testing data to make sure to apply the same transformations. The data can later be separated using whether or not the `Target` `isnull()`. We can then use this table for training and testing a model. The combined training and testing data will be in `data` which we aggregate and eventually join to `household`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1a3ff50986a37c02fd7dbba994c7ebb47451724"
   },
   "source": [
    "\n",
    "\n",
    "### Average Education vs Poverty by Gender\n",
    "\n",
    "Let's extract the labels from the dataset so we have them for later training. While we're at it, we can make a boxplot showing the `meaneduc` versus the value of the `Target` colored by the `gender` of the head of household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f4646bf438dca4cbf50faa61e999090c2346fdb"
   },
   "outputs": [],
   "source": [
    "# Extract the labels\n",
    "label_df = train[train['parentesco1'] == 1].copy()\n",
    "\n",
    "# Create a gender mapping\n",
    "label_df['gender'] = label_df['male'].replace({1: 'M', 0: 'F'})\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(x = 'Target', y = 'meaneduc', hue = 'gender', data = label_df)\n",
    "plt.title('Mean Education vs Target by Gender');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e6adda540eb1e495c5bdde1893dd2328db3a5cb"
   },
   "source": [
    "We can see the negative relationship between average education and poverty (households with higher average levels of education have less sever poverty) as well as a slight gain in average education if the head of household is a female. Plots can often hide the details, so let's do a numeric summary of the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b21fe13a14557a69a069cce2df51ed0da3bd175"
   },
   "outputs": [],
   "source": [
    "label_df.groupby(['gender', 'Target'])['meaneduc'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "047de05a6279eafce4ddc9e334fd1c0572c92f5d"
   },
   "source": [
    "At all poverty levels, the households with female heads have higher levels of average education. I'm not quite sure what to make of this, but it seems right to me! A boxplot does not show the number of observations at each y-value, so for a better visualization that takes into account the number of observations, we can use a violin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe438862ca8aca6527cbadfeba45f285a15ec3b8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "sns.violinplot(x = 'Target', y = 'meaneduc',\n",
    "               hue = 'gender', data = label_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c267a123802ebc2d6bd335abcad334f35306a070"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a8091efe4f67448547b2130ed07dea61ba6a4b7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "052538b57eec42c58efd617449b412d3deeaba3a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c1ba86411116837f233a020171b05db91b9bd8b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a7884df4fcccb3303cd8e4ef467b9f6d3fb75b1"
   },
   "source": [
    "That was a lot of work but we only have to do it once! \n",
    "\n",
    "We'll do two quick checks to make sure we didn't add any variables twice and to make sure we included all of the variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "432d0ca163e272c9f85f81619e2fe7ad372a0961"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf9371ed4b0a09b562e36f6ea797cd0e9680912f"
   },
   "outputs": [],
   "source": [
    "len(x) == len(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d29c05ebf16b88b10622b5bcefb816675a7ccb1"
   },
   "source": [
    "After we define the columns, we need to figure out what to do with all of them. Keep in mind that our approach should probably change as we work with this dataset and I'm open to any suggestions! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90e5be830902d10ee440a82ec0ac47f0be34bb84"
   },
   "outputs": [],
   "source": [
    "test['Target'] = np.nan\n",
    "data = train.append(test)\n",
    "print('Data shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "974a16a001a3e9c5b96614eee13557c369ae2fbf"
   },
   "outputs": [],
   "source": [
    "household = label_df.copy()\n",
    "# Add the test data as an append\n",
    "household = household.append(test[test['parentesco1'] == 1.0], sort = True)\n",
    "# Remove the squared columns\n",
    "household = household.drop(columns = sqr_)\n",
    "# Keep only the household level variables\n",
    "household = household[id_ + hh_bool + hh_cont + hh_ordered].copy()\n",
    "\n",
    "household.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f16b1371fa188ac8722cf5f0d2770a728b066f0"
   },
   "source": [
    "At this point, the `household` table holds all the household level variables as well as the `Target` and the ids. The individual level data is in `data` which we'll get to in a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "771c61d7e7bba4a8fb7be81af632184b7a4196db"
   },
   "source": [
    "## Correcting Object Type Columns\n",
    "\n",
    "We still need to deal with the `object` type columns. Three of these are `dependency`, `edjefe`, and `edjefa`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2d80ceae33a21820f56c8f4a46e2f348904eb76"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6da256b7c235136a44f9c8357097657e73f6a49"
   },
   "source": [
    "Let's visualize the distribution of these variables colored by the `Target`. Below are violin plots of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "596e6918b35c759142b1b96d77634c0b084459ee"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (24, 6))\n",
    "\n",
    "# Iterate through the variables\n",
    "for i, var in enumerate(['dependency', 'edjefa', 'edjefe']):\n",
    "    ax = plt.subplot(1, 3, i+ 1)\n",
    "    # Violinplot colored by `Target`\n",
    "    sns.violinplot(x = 'Target', y = var, ax = ax,\n",
    "                   data = household[household['Target'].notnull()], hue = 'Target');\n",
    "    plt.title(f'{var.capitalize()} by Target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e88ec34d73802ac49769415317b377960114e9bc"
   },
   "source": [
    "We can see that a higher `edjefe` and `edjefa` correspond to higher values of the `Target` (lower levels of poverty). It's hard to determine the effect of the dependency on the `Target` from the plot alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d16a374bc37c67c427a2dac99d5be12fed49ee3"
   },
   "source": [
    "# Baseline Machine Learning Model\n",
    "\n",
    "At this point, we can already get started with the machine learning (we'll shortly come back to using the individual level features.) This model can serve _as a baseline_. It's important to establish a baseline model relatively early on in the data science pipeline so you know if the changes you make are having a positive effect on performance. \n",
    "\n",
    "For our baseline model, we will use _only the household level data_ and a Random Forest Classifier in Scikit-Learn. This won't get us to the top of the leaderboard, but it will allow us to see how well we can do using only a simple set of features. \n",
    "\n",
    "To assess our model, we'll use 10-fold cross validation on the training data. This will essentially _train and test the model 10 times_ using different splits of the training data. 10-fold cross validation is an effective method for estimating the performance of a model on the test set. We want to look at the average performance in cross validation as well as the standard deviation to see how much scores change between the folds. We use the `F1 Macro` measure to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6f26316452144bfeff36474c36c32d1334668ba"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Labels for training\n",
    "train_labels = np.array(list(household[household['Target'].notnull()]['Target'].astype(np.uint8)))\n",
    "\n",
    "# Extract the training data\n",
    "household_train = household[household['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "household_test = household[household['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44fc64eb3f3be49735f6198209b0971e2bc9e072"
   },
   "source": [
    "Because we are going to be comparing different models, we want to scale the features (limit the range of each column to between 0 and 1). For many ensemble models this is not necessary, but when we use models that depend on a distance metric, such as KNearest Neighbors or the Support Vector Machine, feature scaling is an absolute necessity. When comparing different models, it's always safest to scale the features. We also impute the missing values with the median of the feature.\n",
    "\n",
    "For imputing missing values and scaling the features in one step, we can make a pipeline. This will be fit on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4459fe1d50077ba09995fb01664ade706905d10"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('imputer', Imputer(strategy = 'median')), \n",
    "                      ('scaler', MinMaxScaler())])\n",
    "\n",
    "# Fit and transform training data\n",
    "household_train = pipeline.fit_transform(household_train)\n",
    "household_test = pipeline.transform(household_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d288a06b9b05d4868c3771e1e35c7658eb678e3f"
   },
   "source": [
    "Now we can perform 10-fold cross validation on the training data. We'll make a custom scorer so we can use the `macro` F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1b0fde920b9cb7caa2653e417b9cfb7cb3db075"
   },
   "outputs": [],
   "source": [
    "scorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')\n",
    "\n",
    "model = RandomForestClassifier(random_state=10, \n",
    "                               n_estimators=100, n_jobs = -1)\n",
    "# 10 fold cross validation\n",
    "cv_score = cross_val_score(model, household_train, train_labels, cv = 10, scoring = scorer)\n",
    "\n",
    "print(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b68367d25404464a1e62f49eab871f4fd97d75e8"
   },
   "source": [
    "That score is not very good, but it's only a baseline! It leaves us plenty of room to improve. After adding the individual level features, we can re-evaluate the model. We'll also use several different models to make sure we are making the correct choice of model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab7cf09e205b414afb90e5d351f32600b829cfe1"
   },
   "source": [
    "# Individual Level Data\n",
    "\n",
    "The ordinal variables in the individual level data will simply be aggregated. These variables have an inherent ordering, so it makes sense to aggregate them by finding stats such as the `mean`, `min`, and `max`. \n",
    "\n",
    "### Ordered Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "709903ebe79b5cb34768ce182f86d9ae45339107"
   },
   "outputs": [],
   "source": [
    "ind_agg_num = data.groupby('idhogar')[ind_ordered].agg(['mean', 'max', 'min', 'sum'])\n",
    "\n",
    "new_cols = []\n",
    "for col in ind_agg_num.columns.levels[0]:\n",
    "    for stat in ind_agg_num.columns.levels[1]:\n",
    "        new_cols.append(f'{col}-{stat}')\n",
    "\n",
    "ind_agg_num.columns = new_cols\n",
    "ind_agg_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a48ec5d72d4b4b7cb78769fe5cd08faa0104e235"
   },
   "source": [
    "`escolari` is the average schooling so `escolari-mean` is the average schooling of members in the family. This is different than `meaneduc` which is the average education of adults in the household. Let's graph this variable versus the age to see if there is a relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "153a79080e37a926dcd41c23c62e9e2ae73cb3ef"
   },
   "outputs": [],
   "source": [
    "sns.lmplot('escolari-mean', 'age-mean', data = ind_agg_num, size = 6);\n",
    "plt.title('Average Age vs Average Education');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3bcb60ffb23bf209f69bb07482ccf36cba6866b"
   },
   "source": [
    "It looks like there might be a slightly positive relationship between average age of family members and the average education. Let's look at the distribution of the average years behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c8df9534849a35d8cfe4cd1c612fe64526dc2db"
   },
   "outputs": [],
   "source": [
    "ind_agg_num['rez_esc-mean'].round(2).value_counts().sort_index().plot.bar(color = 'red', figsize = (16, 6));\n",
    "plt.title('Average Years Behind'); plt.xlabel('Years Behind'); plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a7bc16905a0ad6281a8e053fcb5ba5291de5e2d5"
   },
   "source": [
    "This might be better as a kernel density estimate plot, but here we can see the two peaks clearly at 0.0 and 1.0. \n",
    "Later, we will look for more trends in the data when we examine correlations between variables and with the `Target`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2bc37d1f8942ee5f9e53b5006bd4a213456c45df"
   },
   "source": [
    "### Boolean Variables\n",
    "\n",
    "The boolean variables won't get the same aggregations: the max of a boolean is just 1 and the min is just 0. Instead, we can take the `sum` of the boolean and the `mean` of the boolean. This will give us both the total number of individuals meeting that criterion in the household and the normalized (divided by the total) total number of individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea0da40cdf8e0e76ad4e0a855aad883ee30ed31e"
   },
   "outputs": [],
   "source": [
    "train[['v18q', 'v18q1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6be07585c07b85565324567d0b20395b382c78f5"
   },
   "outputs": [],
   "source": [
    "ind_agg_bool = data.groupby('idhogar')[ind_bool].agg(['mean', 'sum'])\n",
    "\n",
    "new_cols = []\n",
    "for col in ind_agg_bool.columns.levels[0]:\n",
    "    for stat in ind_agg_bool.columns.levels[1]:\n",
    "        new_cols.append(f'{col}-{stat}')\n",
    "\n",
    "ind_agg_bool.columns = new_cols\n",
    "ind_agg_bool.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1dcc9ec6bd33962879c9a84692753cbfc9cc5946"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 6))\n",
    "plt.subplot(131)\n",
    "ind_agg_bool['male-sum'].value_counts().sort_index().plot.bar(color = 'green')\n",
    "plt.title('Number of Males Distribution')\n",
    "plt.subplot(132)\n",
    "ind_agg_bool['female-sum'].value_counts().sort_index().plot.bar(color = 'orange')\n",
    "plt.title('Number of Females Distribution');\n",
    "plt.subplot(133)\n",
    "sns.kdeplot(ind_agg_bool['female-sum'] / ind_agg_bool['male-sum'])\n",
    "plt.title('Female:Male Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3641b8d932a431adcf533507b2180589eab8f3fe"
   },
   "source": [
    "The final thing we want to do is simply count the number of individuals in each household. The bar plot shows the distribution (a bar plot is fine in this case because the variable is ordinal and not continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef34caafd154d72c056d5e1a4cbfe3a9ed14a67a"
   },
   "outputs": [],
   "source": [
    "# Count number of individuals in each household\n",
    "ind_counts = data.groupby('idhogar')['escolari'].count()\n",
    "\n",
    "ind_counts.value_counts().sort_index().plot.bar(color = 'red');\n",
    "plt.title('Count of Individuals by Household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da381ed51485296ccf7c279665c5c141d640ff82"
   },
   "source": [
    "### Join to the household data\n",
    "\n",
    "The individual level data can now be merged with the household labeled data on `idhogar` (this will also remove any households that did not have a head of household that we still aggregated). After the merging, we'll have many more features for modeling and for data exploration! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "226f2cdf7f37b30d34c0829a958a07a34f028fe9"
   },
   "outputs": [],
   "source": [
    "print('Original Features: ', household.shape[1])\n",
    "household = household.merge(ind_agg_num, on = 'idhogar', how = 'left')\n",
    "print('Features after numeric individual data: ', household.shape[1])\n",
    "household = household.merge(ind_agg_bool, on = 'idhogar', how = 'left')\n",
    "print('Features after boolean individual data: ', household.shape[1])\n",
    "household = household.merge(pd.DataFrame(ind_counts).rename(columns = {'escolari': 'count'}), on = 'idhogar', how = 'left')\n",
    "print('Features after all individual data: ', household.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "327b217d5d7df0f6aceb9181302408eea505e6eb"
   },
   "source": [
    "Later we'll want to try some __feature selection__ to remove unnecessary columns. At the moment however, we'll just take two simple feature selection steps:\n",
    "\n",
    "1. Remove any one of any pair of columns with a correlation greater than 0.99\n",
    "2. Remove any columns with only a single unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed84f3d1e01d669d41e6b682e3054876ded8a840"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = household.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 0.99)]\n",
    "\n",
    "print('There are {} columns with >= 0.99 correlation.'.format(len(to_drop)))\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a032e7037f4e0b3584435cbe5ddfbf00bb0d4e04"
   },
   "outputs": [],
   "source": [
    "household.corr()['count'].sort_values().tail(6).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2462363e677235e8505826bd32d013e85d23e621"
   },
   "source": [
    "Well, it looks like we didn't need to add the counts! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4a15ee224cef5a5da4b944ce1ebb99d7888a84e"
   },
   "source": [
    "We're checking for absolute correlations close to 1.0 which means that `female-mean` and `male-mean` are perfectly negatively correlated: by definition, they have to sum to one! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8b0e13319fbd92a74de7bbe0fba337a9b0255b6"
   },
   "outputs": [],
   "source": [
    "sns.lmplot('female-mean', 'male-mean', data = household);\n",
    "plt.title('Average Female vs Average Male');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "893012d844a1737f0943c9122aca044a0b7d9eec"
   },
   "source": [
    "This serves as a sanity check that we correctly calculated the statistic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91de9df0a125753c8ea2f56ffa65cdd4b7738910"
   },
   "outputs": [],
   "source": [
    "print(household['female-mean'].corr(household['male-mean']).round(5))\n",
    "\n",
    "#Drop the columns\n",
    "household = household[[x for x in household if x not in to_drop]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5338542d849eb1ba0b6bcb15099192ceaecbe219"
   },
   "source": [
    "Now for the columns with a single unique value. A column with only one unique value will not be useful for modeling since it contains 0 information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02bb61618de8ab64ef3843b2bdc0076fa2332b84"
   },
   "outputs": [],
   "source": [
    "household.columns[np.where(household.nunique() == 1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79f4864f6ba9b2bf4bb3ffe848846a536d07477f"
   },
   "source": [
    "`parentesco1-sum` makes a lot of sense because each household will have exactly one head! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1829e1b74517e77df5c2327a906c3e756e29cf2c"
   },
   "outputs": [],
   "source": [
    "household = household.drop(columns = ['elimbasu5', 'parentesco1-sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a7e901ed5b3144a426547e0a040fe334ea28b3c"
   },
   "source": [
    "## Evaluate Model with Individual Data\n",
    "\n",
    "We can now use the expanded set of features enriched with the individual level data to retrain the model and evaluate in cross validation. Hopefully the scores improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78acad2b5eb0cc49b6d735dcac8275430263bb03"
   },
   "outputs": [],
   "source": [
    "# Extract the trianing data\n",
    "household_train = household[household['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "household_test = household[household['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "\n",
    "# Extract the features for feature importances\n",
    "features = list(household_train.columns)\n",
    "household_train = pipeline.fit_transform(household_train)\n",
    "household_test = pipeline.transform(household_test) \n",
    "\n",
    "# 10 fold cross validation\n",
    "cv_score = cross_val_score(model, household_train, train_labels, cv = 10, scoring = scorer, verbose = 1)\n",
    "print(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d93a45c46832d0b7804a112925d19bb5739e586b"
   },
   "source": [
    "Including the individual data did improve our cross validation scores! Good thing because we went to a lot of hard work to find those.\n",
    "\n",
    "## Feature Importances\n",
    "\n",
    "If we want to view the feature importances, we'll have to train a model on the whole training set. Cross validation does not return the feature importances. Below is a short function we'll use to plot the feature importances. I use this function a lot and often copy and paste it between scripts. I hope the documentation makes sense! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9912b9bdd0204aea423ec6e610f7788726ed038"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, n = 15, threshold = None):\n",
    "    \"\"\"Plots n most important features. Also plots the cumulative importance if\n",
    "    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n",
    "    Intended for use with any tree-based feature importances. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n",
    "    \n",
    "        n (int): Number of most important features to plot. Default is 15.\n",
    "    \n",
    "        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n",
    "        \n",
    "    Returns:\n",
    "        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n",
    "                        and a cumulative importance column\n",
    "    \n",
    "    Note:\n",
    "    \n",
    "        * Normalization in this case means sums to 1. \n",
    "        * Cumulative importance is calculated by summing features from most to least important\n",
    "        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    # Sort features with most important at the head\n",
    "    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # Normalize the feature importances to add up to one and calculate cumulative importance\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    # Bar plot of n most important features\n",
    "    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n",
    "                            x = 'feature', color = 'blue', \n",
    "                            edgecolor = 'k', figsize = (12, 8),\n",
    "                            legend = False, linewidth = 2)\n",
    "\n",
    "    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n",
    "    plt.title(f'Top {n} Most Important Features', size = 18)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    \n",
    "    if threshold:\n",
    "        # Cumulative importance plot\n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n",
    "        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n",
    "        plt.title('Cumulative Feature Importance', size = 18);\n",
    "        \n",
    "        # Number of features needed for threshold cumulative importance\n",
    "        # This is the index (will need to add 1 for the actual number)\n",
    "        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "        \n",
    "        # Add vertical line to plot\n",
    "        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n",
    "        plt.show();\n",
    "        \n",
    "        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n",
    "                                                                                  100 * threshold))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0d04a9495e7be357599a1c91ea196908efd0624"
   },
   "outputs": [],
   "source": [
    "model.fit(household_train, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "fi = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})\n",
    "norm_fi = plot_feature_importances(fi, 15, threshold=.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5403f4fdc5a5bd8192a2ec3a09f062d8573f3c1b"
   },
   "source": [
    "__Education reigns supreme!__ The most important variable is the average amount of education in the household, followed by the average amount of education of the adults in the household.  I have a suspicion these variables are highly correlated (collinear) which means we may want to remove one of them from the data. The other most important features have to do with the age, the dependency rate, overcrowding, and eventually the number of rooms in the house. Most of these intuitively make sense as being related to the poverty of a household! \n",
    "\n",
    "It's interesting that we only need 97 of the ~170 features to account for 90% of the importance. This tells us that we may be able to remove some of the features. However, feature importances don't tell us which direction of the feature is important (for example, we can't use these to tell whether more or less education leads to more severe poverty) they only tell us which features the model considered relevant. To assess the directionality relationship of features, we can use correlations between the features and the Target.\n",
    "\n",
    "# Exploring Correlations\n",
    "\n",
    "While the correlation coefficient only measures _linear relationships_ between two variables, it can still be useful for identifying variables that are related. If we calculate the correlations of variables with the `Target`, this can serve as a first approximation of what varibles will be useful in a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e92aa0a5b4d95276506a399886e7086a1361c40b"
   },
   "outputs": [],
   "source": [
    "corrs = household[household['Target'].notnull()].corr()['Target'].sort_values()\n",
    "corrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef4330d14e5247af58f9c064f97048d200ec41ec"
   },
   "source": [
    "These are the most _negatively_ correlated variables with the `Target`, meaning that as they increase, the `Target` decreases. Since the lower the target the greater the level of poverty, an increase in these variables means an increase in poverty. Basically, a family with higher levels of these variables is more at risk for extreme poverty. \n",
    "\n",
    "For example, a higher total number of `instlevel2` individuals correspond to more poverty. This variable represents that the individual _did not complete primary_ school, showing once again that education is inversely related to poverty. Another actionable insight in the correlations is that more children `hogar_nin` corresponds to greater poverty. Insights such as these are what make data science valuable: if we have the resources, we can act on these findings to improve real-world outcomes.\n",
    "\n",
    "Next we can look at the most _positive_ correlations with the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "617e0bf3216902389414e652e1a6a86c066e8554"
   },
   "outputs": [],
   "source": [
    "corrs.dropna().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c317c57b3bf16a436639b5ffb46f1e255d615b1"
   },
   "source": [
    "As these variables increase, the poverty tends to decrease. And what do we see on top: education of course! If we had to make one conclusion so far, it's __education is inversely related to poverty.__ We don't know if higher levels of education cause less poverty or if more poverty leads to lower levels of education (it's probably a mix of both) but we do know that these two variables are related. \n",
    "\n",
    "The general guidelines for correlation values are below, but these will change depending on who you ask ([source](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf) for these):\n",
    "\n",
    "*  .00-.19 very weak\n",
    "*  .20-.39 weak\n",
    "*  .40-.59 moderate\n",
    "*  .60-.79 strong\n",
    "*  .80-1.0 very strong\n",
    "\n",
    "## Plots of Correlations\n",
    "\n",
    "Let's look at a few of these variables. The following plots are kernel density estimate plots that show the distribution of a single variable. To see how these change with the povery level, the lines will be colored the `Target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bba6ca1392bc448030135fd7e013465b7ae2bb1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "variable = 'meaneduc'\n",
    "colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}\n",
    "\n",
    "# Plot each unique level of the target\n",
    "for level in household[household['Target'].notnull()]['Target'].unique():\n",
    "    subset = household[(household['Target'].notnull()) & (household['Target'] == level)].copy()\n",
    "    sns.kdeplot(subset[variable].dropna(), \n",
    "                label = f'Poverty Level: {level}', \n",
    "                color = colors[subset['Target'].unique()[0]],\n",
    "                linewidth = 2)\n",
    "    \n",
    "plt.xlabel(variable); plt.ylabel('Density');\n",
    "plt.title('{} Distribution'.format(variable.capitalize()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e02d1509f70917f7f8a760cbcec36ea917ac4ef",
    "collapsed": true
   },
   "source": [
    "`meaneduc` is the average years of education for adults in the household, so this relationship intuitively makes sense: [greater levels of education generally correlate with lower levels of poverty](https://www.childfund.org/poverty-and-education/). We don't necessarily know which causes which, but we do know these tend to move in the same direction.\n",
    "\n",
    "We might want to use the above code multiple times, so let's put it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "540771805485f245fa5a0a7d18f5fc35b7169e2a"
   },
   "outputs": [],
   "source": [
    "def kde_target(df, variable):\n",
    "    \"\"\"Plots the distribution of `variable` in `df` colored by the `Target` column\"\"\"\n",
    "    \n",
    "    colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}\n",
    "\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    \n",
    "    \n",
    "    for level in df['Target'].unique():\n",
    "        subset = df[df['Target'] == level].copy()\n",
    "        sns.kdeplot(subset[variable].dropna(), \n",
    "                    label = f'Poverty Level: {level}', \n",
    "                    color = colors[subset['Target'].unique()[0]])\n",
    "\n",
    "    plt.xlabel(variable); plt.ylabel('Density');\n",
    "    plt.title('{} Distribution'.format(variable.capitalize()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5b3a8442847118073b35dca3502112b0543630a"
   },
   "source": [
    "We can use this to look at one of the negative correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3db2aab481d2c7a641b722c87ef9ff9eded0d102"
   },
   "outputs": [],
   "source": [
    "kde_target(household[household['Target'].notnull()], 'hogar_nin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3dd7e832c116a3dc5316bd03862e3f3624f18f86"
   },
   "source": [
    "The `hogar_nin` is the number of children 0 - 19 in the family which also makes sense: younger children can be financial source of stress on a family leading to higher levels of poverty. Or, families with lower socioeconomic status have more children in the hopes that one of them will be able to succeed. Whatever the explanation, there is a [real link between family size and poverty](https://www.adb.org/sites/default/files/publication/157217/adbi-rp68.pdf)\n",
    "\n",
    "A better way to show this plot is as a bar plot because the family size is discrete (an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7e31cdee82fed3ba76d99ed090eda56e3854f90"
   },
   "outputs": [],
   "source": [
    "household[household['Target'].notnull()].groupby('hogar_nin')['Target'].mean().plot.bar(color = 'blue', figsize = (8, 6));\n",
    "plt.xlabel('Number of Children'); plt.ylabel('Average Target');\n",
    "plt.title('Average Poverty Level by Number of Children', size = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e06cdc955dd8c5d7b8f246aabc9ad91e904c7ea7"
   },
   "source": [
    "A boxplot might also come in handy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e60028644098d72096df3a4b221acc51f808c0c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "sns.boxplot(x = 'hogar_nin', y = 'Target', data = household[household['Target'].notnull()]);\n",
    "plt.title('Target Distribution by Number of Children');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "026519c86fbec894c4bc48107a80a051e2cfa8b2"
   },
   "source": [
    "It turns out the boxplot is not a great way to visualize a discrete variable on the y-axis. Let's turn this around and make it into a swarmplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e52ae5e587dd00e72375de1bb6cc63cc7b8a05c8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "sns.violinplot(y = 'hogar_nin', x = 'Target', data = household[household['Target'].notnull()]);\n",
    "plt.title('Target Distribution by Number of Children');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c79659bb6145cc4a8e6c45953366e1506ad694e9"
   },
   "source": [
    "There is an unmistakeable trend: as family size increases, the target decreases indicating the severity of poverty increases. The jump in average poverty level `Target` at 7 is interesting, and might be caused by a small sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24dc9b317e91f5400614cbe26242a8f0aa588ade"
   },
   "outputs": [],
   "source": [
    "household[household['Target'].notnull()].groupby('hogar_nin')['Target'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7949448df0c3fdaf4fb65d2281927eff2a421261"
   },
   "source": [
    "The seven children family only occurs twice in the data. This illustrates a key point: __be careful about interpreting graphs without looking at the underlying data__. The violinplot may be more accutate in this case because the width of the graph indicates the number of observations occuring at the y-value. Therefore we can see there are relatively few observations of households with a large number of children. It's usually a __good idea to try several plots and also consider the stats themselves when analyzing data.__\n",
    "\n",
    "The next plot shows the most important variable, the average education of everyone in the household. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da7cecb7c529ebc86a19127c194a0f624d73b403"
   },
   "outputs": [],
   "source": [
    "kde_target(household[household['Target'].notnull()], 'escolari-mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1299d9fb194bcb0cfceaa5f1ef22b50d346b6061"
   },
   "source": [
    "#### Correlation Heatmap \n",
    "\n",
    "One of my favorite plots is the correlation heatmap because it shows a ton of info in one image. For the heatmap, we'll pick 8 variables and show the correlations between themselves and with the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58424e2bdebd4559bf62b737dd27e36c29db7568"
   },
   "outputs": [],
   "source": [
    "variables = ['instlevel2-sum', 'instlevel1-sum', 'hogar_nin', 'meaneduc', 'r4t1', 'age-mean',\n",
    "             'escolari-max', 'escolari-mean', 'Target']\n",
    "\n",
    "# Calculate the correlations\n",
    "corr_mat = household[household['Target'].notnull()][variables].corr().round(2)\n",
    "\n",
    "# Draw a correlation heatmap\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize = (14, 14))\n",
    "sns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n",
    "            cmap = plt.cm.RdYlGn_r, annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c1d092ba64d4fbfbf85dd94f7c0e3a96b1596db"
   },
   "source": [
    "Feel free to spend as much time taking in the plot as you need! We can immediately notice some trends:\n",
    "\n",
    "* The `meaneduc` and `escolari-mean` have a high correlation with one another as expected.\n",
    "* `escolari-mean` and `escolari-max` are also highly correlated as we could have guessed! \n",
    "* `escolari-mean` has the most positive correlation of any variable with the `Target`\n",
    "* `instlevel2-sum` has the most negative correlation of any variable with the target\n",
    "\n",
    "### Feature Plots\n",
    "\n",
    "To visualize what the correlation represent, we can use a pairplot which shows scatterplot, kdeplots, and 2D density estimate plots. This uses the `sns.PairGrid` function to show `scatter` plots on the upper triangle, `kdeplot`s on the main diagonal, and 2D kernel density estimate plots on the lower triangle.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c0c3ae8a04a8a68501e6609b50bd7c13c4a9c12"
   },
   "outputs": [],
   "source": [
    "# Copy the data for plotting\n",
    "plot_data = household[household['Target'].notnull()][['instlevel2-sum', 'hogar_nin', 'Target', \n",
    "                                                        'r4t1', 'escolari-mean']]\n",
    "\n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data = plot_data, size = 4, diag_sharey=False,\n",
    "                    hue = 'Target', hue_order = [4, 3, 2, 1], \n",
    "                    vars = [x for x in list(plot_data.columns) if x != 'Target'])\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha = 0.2)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
    "grid = grid.add_legend()\n",
    "plt.suptitle('Feature Plots Colored By Target', size = 32, y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ed8c4d094d1e8bed0d784f935124045158c9382"
   },
   "source": [
    "Besides being nice to look at, these plots can be used to inform our modeling decisions or to find insights which might not jump out at us in the quantitative summaries. We already did quite a bit of analysis, so the info in this plot might not be new, but sometimes there is no alternative to getting a look at your data! \n",
    "\n",
    "One issue that we can run into when creating variables is highly correlated variables which are called __collinear__. These can slow down model training (in general more features means a slower model to train), decrease model interpretability, and decrease model generalization to the test set. We already removed variables with a perfect correlation (that is they have all the same exact values), but there may still be highly correlated columns we can address later when we do more feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb3574b0f4d56a904729cc7c800e5619e3e89e8f"
   },
   "source": [
    "# Model Selection\n",
    "\n",
    "Now that we have a good set of features, it's time to get into the modeling. We already tried one basic model, the Random Forest Classifier which delivered a best macro F1 of 0.35. However, in machine learning, there is no way to know ahead of time which model will work best for a given dataset. The following plot shows that __there are some problems where even Gaussian Naive Bayes will outperform a gradient boosting machine__. This is from [an excellent paper by Randal Olson that discusses many points of machine learning](https://psb.stanford.edu/psb-online/proceedings/psb18/olson.pdf)\n",
    "\n",
    "![algorithm_comparison](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/algorithm_comparison.png)\n",
    "\n",
    "What this plot tells us is that we have to try out a number of different models to see which is optimal. Most people eventually settle on the __gradient boosting machine__ and we will try that out, but for now we'll take a look at some of the other options.  There are literally dozens (maybe hundreds) of multi-class machine learning models if we look at the [Scikit-Learn documentation](http://scikit-learn.org/stable/modules/multiclass.html). We don't have to try them all, but we should sample from the options.\n",
    "\n",
    "What we want to do is write a function that can evaluate a model. This will be pretty simple since we already wrote most of the code. In addition to the Random Forest Classifier, we'll try eight other Scikit-Learn models. Luckily, this dataset is relatively small and we can rapidly iterate through the models. We will make a dataframe to hold the results and the function will add a row to the dataframe for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78a7710f39bab271c4320f42b9bbc7e3f9a4bd2f"
   },
   "outputs": [],
   "source": [
    "# Model imports\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "665cd410e664a6092b756a888727f166fbb5a5ed"
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Filter out warnings from models\n",
    "warnings.filterwarnings('ignore', category = ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category = DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "# Dataframe to hold results\n",
    "model_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n",
    "\n",
    "def cv_model(train, train_labels, model, name, model_results=None):\n",
    "    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n",
    "    \n",
    "    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n",
    "    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n",
    "    \n",
    "    if model_results is not None:\n",
    "        model_results = model_results.append(pd.DataFrame({'model': name, \n",
    "                                                           'cv_mean': cv_scores.mean(), \n",
    "                                                            'cv_std': cv_scores.std()},\n",
    "                                                           index = [0]),\n",
    "                                             ignore_index = True)\n",
    "\n",
    "        return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab07945c331294a68d600de58e4ed9c2739473bf"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(household_train, train_labels, LinearSVC(), \n",
    "                         'LSVC', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ea1ec18bda63d33c6e6482de5361d1728f41bbd"
   },
   "source": [
    "That's one model to cross off the list (although we didn't perform hyperparameter tuning so the actual performance could possibly be improved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5c13ea9fb13134e3b25d59bedf5b9f974e1517c"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(household_train, train_labels, \n",
    "                         GaussianNB(), 'GNB', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a73eca1b3b7be54e3d6bae663169e67854f91a06"
   },
   "source": [
    "That performance is very poor. I don't think we need to revisit the Gaussian Naive Bayes method (although there are problems on which it can outperform the Gradient Boosting Machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b1dca57fa07e406f1fe81fa320b3b97ab078d8e"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(household_train, train_labels, \n",
    "                         MLPClassifier(hidden_layer_sizes=(16, 32, 64, 64, 32)),\n",
    "                         'MLP', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84fb16fdda905c410ef60cef95016c85306fe81c"
   },
   "source": [
    "The multi-layer perceptron (a deep neural network) has decent performance. This might be an option if we are able to hyperparameter tune the network. However, the limited amount of data could be an issue with a neural network as these generally require hundreds of thousands of examples to learn effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2310f6d79426e16c9e3af6f8d4bfe03aa847d1b7"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(household_train, train_labels, \n",
    "                          LinearDiscriminantAnalysis(), \n",
    "                          'LDA', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44862a70f227c4a2ec0b73431f0fdeeeebf09f0a"
   },
   "source": [
    "__If you run `LinearDiscriminantAnalysis` without filtering out the `UserWarning`s, you get many messages saying \"Variables are collinear.\"__ This might give us a hint that we want to remove some collinear features! We might want to try this model again after removing the collinear variables because the score is comparable to the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c5c3d651e96cfe01af34f93e6faa501fa35a94f"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(household_train, train_labels, \n",
    "                         RidgeClassifierCV(), 'RIDGE', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94459bb16bb7ddf3f49ee1a547818dd52efbe764"
   },
   "source": [
    "The linear model (with ridge regularization) does surprisingly well. This might indicate that a simple model can go a long way in this problem (although we'll probably end up using a more powerful method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e753af20c3aa36a515175fe7ce91a894ee00b512"
   },
   "outputs": [],
   "source": [
    "# model_results = cv_model(household_train, train_labels, \n",
    "#                          QuadraticDiscriminantAnalysis(), 'QDA', model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b10b93b0caed61b6b54df8acfdda3e144c6499cf"
   },
   "outputs": [],
   "source": [
    "for n in [5, 10, 20]:\n",
    "    print(f'\\nKNN with {n} neighbors\\n')\n",
    "    model_results = cv_model(household_train, train_labels, \n",
    "                             KNeighborsClassifier(n_neighbors = n),\n",
    "                             f'knn-{n}', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49639627d4e1513ca07395b9a471295afc0a03c6"
   },
   "source": [
    "As one more attempt, we'll consider the ExtraTreesClassifier, a variant on the random forest using ensembles of decision trees as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eeab91bfb4b953e0c7aa5112625b65268537c662"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model_results = cv_model(household_train, train_labels, \n",
    "                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n",
    "                         'EXT', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f4a3dc32b62b3fcc625e54f8d92afd9f1de2e7d"
   },
   "source": [
    "## Comparing Model Performance\n",
    "\n",
    "With the modeling results in a dataframe, we can plot them to see which model does the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8c2a9a2df39625e6b1066ca950e44b5fe68862e"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(household_train, train_labels,\n",
    "                          RandomForestClassifier(100, random_state=10),\n",
    "                              'RF', model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cbd9d6729288337c5c3cace44af254ebd480fce"
   },
   "outputs": [],
   "source": [
    "model_results.set_index('model', inplace = True)\n",
    "model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n",
    "                                  yerr = list(model_results['cv_std']))\n",
    "plt.title('Model F1 Score Results');\n",
    "plt.ylabel('Mean F1 Score (with error bar)');\n",
    "model_results.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "876ff3ad8202066da47d7aea5d1784a981c81edb"
   },
   "source": [
    "The most likely candidate seems to be the random forest because it does best right out of the box.  While we didn't tune any of the hyperparameters so the comparison between models is not perfect, these results reflect those of many other Kaggle competitiors finding that tree-based ensemble methods (including the Gradient Boosting Machine) perform very well on structured datasets. Hyperparameter performance does improve the performance of machine learning models, but we don't have time to try all possible combinations of settings for all models. The graph below ([from the paper by Randal Olson](https://psb.stanford.edu/psb-online/proceedings/psb18/olson.pdf)) shows the effect of hyperparameter tuning versus the default values in Scikit-Learn.\n",
    "\n",
    "![hyperparameter_improvement](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/hyperparameter_improvement.png)\n",
    "\n",
    "In most cases the accuracy gain is less than 10% so the worst model is probably not suddenly going to become the best model through tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e9b271483c048ab27e584577a94f0dc4a2e47e6"
   },
   "source": [
    "For now we'll say the random forest does the best. Later we'll look at using the Gradient Boosting Machine, although not implemented in Scikit-Learn. Instead we'll be using the more powerful [LightGBM version](http://lightgbm.readthedocs.io/en/latest/). Now, let's turn to making a submission using the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de34b3435f0ff50a2c437799867de0689fc91c7c"
   },
   "source": [
    "# Making a Submission\n",
    "\n",
    "In order to make a submission, we need the test data. Fortunately, we have the test data formatted in exactly the same manner as the train data. \n",
    "\n",
    "The format of a testing submission is shown below. Although we are making predictions for each household, we actually need one row per individual (identified by the `Id`) but only the prediction for the head of household is scored. \n",
    "\n",
    "```\n",
    "Id,Target\n",
    "ID_2f6873615,1\n",
    "ID_1c78846d2,2\n",
    "ID_e5442cf6a,3\n",
    "ID_a8db26a79,4\n",
    "ID_a62966799,4 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e38e6c120980df2d11f1fc5a2377b674b066481"
   },
   "source": [
    "The `submission_base` will have all the individuals in the test set since we have to have a \"prediction\" for each individual while the `test_ids` will only contain the `idhogar` from the heads of households. When predicting, we only predict for the heads of households and then we merge the `predictions` dataframe with all of the individuals on the household id (`idhogar`). This will set the `Target` to the same value for everyone in a household. For the test households without a head of household, we can just set these predictions to 4 since they will not be scored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ce052801e31cfbfd83ce2af021fd8e06e90b8c3"
   },
   "outputs": [],
   "source": [
    "submission_base = test.loc[:, ['idhogar', 'Id']]\n",
    "submission_base['idhogar'] = submission_base['idhogar']\n",
    "\n",
    "# The tests ids are only for the heads of households.\n",
    "test_ids = list(test.loc[test['parentesco1'] == 1, 'idhogar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c4072b5bb0210abddfd1652eb123f988048c081"
   },
   "source": [
    "The function below takes in a model, a training set, the training labels, and a testing set and performs the following operations:\n",
    "\n",
    "* Trains the model on the training data using `fit`\n",
    "* Makes predictions on the test data using `predict`\n",
    "* Creates a `submission` dataframe that can be saved and uploaded to the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "444972fb20122f5a73f868789ecd170e2ffdced8"
   },
   "outputs": [],
   "source": [
    "def submit(model, train, train_labels, test):\n",
    "    \"\"\"Train and test a model on the dataset\"\"\"\n",
    "    \n",
    "    # Train on the data\n",
    "    model.fit(train, train_labels)\n",
    "    predictions = model.predict(test)\n",
    "    predictions = pd.DataFrame({'idhogar': test_ids,\n",
    "                               'Target': predictions})\n",
    "\n",
    "     # Make a submission dataframe\n",
    "    submission = submission_base.merge(predictions, \n",
    "                                       on = 'idhogar',\n",
    "                                       how = 'left').drop(columns = ['idhogar'])\n",
    "    \n",
    "    # Fill in households missing a head\n",
    "    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n",
    "\n",
    "    return submission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5107d84e364842331dd23e3112835646e22ce004"
   },
   "source": [
    "Let's make a submission with the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0eeba88ccae80a22fad6d62ce924f6b517183d6"
   },
   "outputs": [],
   "source": [
    "rf_submission = submit(RandomForestClassifier(n_estimators = 100, \n",
    "                                              random_state=10, n_jobs = -1), \n",
    "                         household_train, train_labels, household_test)\n",
    "rf_submission.to_csv('rf_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eee428be91dd21820654f91b9225384b3d7f8527"
   },
   "source": [
    "These predictions score __0.370__ when submitted to the competition.\n",
    "\n",
    "We can visualize the distribution of labels in the test data. This will serve as a sanity check on our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f7c53c96da69065d37cb4fb0399337eabf5b0b2"
   },
   "outputs": [],
   "source": [
    "rf_submission['Target'].value_counts().sort_index().plot.bar(figsize = (8, 6), \n",
    "                                                             color = 'red');\n",
    "plt.title(\"Submission Target Value Distribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84e1621be51031a05b3a08db2291aeb220457ce9"
   },
   "source": [
    "The distribution looks about what we would expect although there appear to be more 4s than in the actual label distribution. We can plot the train label distribution and the predicted label distributed as normed histograms to see if there is a skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "129a633afaad1ee841149eedeff8419e7ddeba2d"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharey = True, figsize = (12, 6))\n",
    "train['Target'].sort_index().plot.hist(normed = True, ax = axes[0])\n",
    "plt.xticks([1, 2, 3, 4]);\n",
    "axes[0].set_title('Train Label Distribution')\n",
    "\n",
    "rf_submission['Target'].sort_index().plot.hist(normed = True, ax = axes[1])\n",
    "plt.xticks([1, 2, 3, 4]);\n",
    "\n",
    "plt.subplots_adjust()\n",
    "plt.title('Predicted Label Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3624312b801c314657869bdfcec97beca9c3296c"
   },
   "source": [
    "It looks like 4 is overrepresented in the predictions and the other labels are underrepresented. To try and fix this, we can specify `class_weight = \"balanced\"` in the Random Forest which will re-weight the labels by their relative frequency in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "460403b53e86186da8eb3db55ef3d28da1c224b5"
   },
   "outputs": [],
   "source": [
    "balanced_submission = submit(RandomForestClassifier(n_estimators = 100, class_weight = 'balanced', \n",
    "                                              random_state=10, n_jobs = -1), \n",
    "                         household_train, train_labels, household_test)\n",
    "print('Balanced Label Counts')\n",
    "balanced_submission['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd8c1a78f7be056ea4e2f409074ae36a4a517234"
   },
   "outputs": [],
   "source": [
    "print('Non balanced label counts')\n",
    "rf_submission['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd2cf0202d32ea90750617180c63253a4e26b3d0"
   },
   "source": [
    "Setting class weight to `balanced` _increased_ the number of predicted 4s. This is puzzling and we might want to make a submission with each method and see which does better (although this would technically be tuning with the testing set, which we don't want to do in a real problem). Later, we might look at undersampling the majority class, or [oversampling the minority class](http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html) to get better performance. (A few other competitions have tried this tactic with some success).\n",
    "\n",
    "For now we'll go back to work with the features rather than the model. Notice how this is an iterative procedure: first we do feature engineering and test the model, then we go back and do more feature engineering / feature selection before test the model again. The key point is to measure our performance at each step so we now what improves our model. __Experiment, measure, revise__ is a good workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ece71f2d171f021c667f9bc6ff7b62f6645a3f4e"
   },
   "source": [
    "# Feature Selection\n",
    "\n",
    "Feature selection is one of the most important topics in machine learning and also an area in which there are no well-estalished standards for removing certain columns. Moreover, there are many different techniques to take. Here, we'll focus on a few simple and straightforward methods:\n",
    "\n",
    "1. Removing columns with more than 90% missing values \n",
    "2. Removing one of every pair of columns with correlation greater than 0.95\n",
    "3. Removing zero-importance features from a random forest\n",
    "\n",
    "After these steps, we'll try the modeling again with the Random Forest to see if our cross validation improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de73653ab2ee414200331d98dead7a0b3b621fea"
   },
   "outputs": [],
   "source": [
    "missing = pd.DataFrame(household.isnull().sum()).rename(columns = {0: 'total'})\n",
    "missing['percent'] = missing['total'] / len(household)\n",
    "missing.sort_values('percent', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28f99a6dfeba7473604f2c0218121a947e688c17"
   },
   "source": [
    "It turns out that none of the columns have more than 90% missing values.\n",
    "\n",
    "# Handling Missing Values\n",
    "\n",
    "Earlier we dealt with missing values by simply filling them in with the median value of the column. However, we should revisit that decision by actually considering what the features represent. \n",
    "\n",
    "Let's start with `v18q1` which indicates whether the family owns a tablet. We can look at the value counts of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8586fa20cdaa477de7e740fdebaad025ddb60710"
   },
   "outputs": [],
   "source": [
    "household['v18q1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c6fa3abd40fa0d117fb14b0dc5abc31f274dd96"
   },
   "source": [
    "It looks like the most common number of tablets to own is 1 if we go only by the data that is present. However, we also need to think about the data that is missing: in this case, it could be that families with a `nan` in this category just do not own a tablet! I think the safest bet in this case is to set the missing tablet measurements to 0.  We'll do that in the code below and move on to the next column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b94abcb5642bbcc605652e67bebdf273150600d"
   },
   "outputs": [],
   "source": [
    "household['v18q1'] = household['v18q1'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ece0e7e8b324b2290d062f91ec11c992af654c41"
   },
   "source": [
    "`v2a1` represents the montly rent payment. For the missing values in this case, it will be interesting to look at the distribution of `tipovivi_`, the columns showing the ownership/renting status of the home. For example, maybe for those households with a missing rent payment, the house is owned which would make sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7978edd75c9141d836e9f9a393bb58d2b83b7aa1"
   },
   "outputs": [],
   "source": [
    "# Variables indicating home ownership\n",
    "own_variables = [x for x in household if x.startswith('tipo')]\n",
    "\n",
    "\n",
    "# Plot of the home ownership variables for home missing rent payments\n",
    "household.loc[household['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (8, 6),\n",
    "                                                                        color = 'purple');\n",
    "\n",
    "plt.title('Home Ownership Status for Households Missing Rent Payments', size = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62371b956c4b38f1eddc1e2a3c4123ba813f89c4"
   },
   "source": [
    "The meaning of the home ownership variables is below:\n",
    "\n",
    "    tipovivi1, =1 own and fully paid house\n",
    "    tipovivi2, \"=1 own,  paying in installments\"\n",
    "    tipovivi3, =1 rented\n",
    "    tipovivi4, =1 precarious\n",
    "    tipovivi5, \"=1 other(assigned,  borrowed)\"\n",
    "    \n",
    "We've solved the issue! Well, mostly: the households that do not have a monthly rent payment generally own their own home. In a few other situations, we are not sure of the reason for the missing information. As a solution, for the houses with no rent payment and the houses are owned, we can set the montly rent payment to 0. For the other houses, we'll leave the missing value to be imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29fd69ce5bdf09236ce12fac46df2c71d95b8324"
   },
   "outputs": [],
   "source": [
    "household.loc[(household['tipovivi1'] == 1), 'v2a1'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2de85c5ad1488da59e131b770f8c9b6eeb9e3f88"
   },
   "source": [
    "There are many missing values in the columns derived from `rez_esc` indicating years behind in school. For these families, it's possible there are no children and hence no one is any years behind. Let's test this out using the `hogar_nin` and `r4t1` columns and the missing values in the years behind features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b13ddcd9a366a87b850375c11749ace4e9985cec"
   },
   "outputs": [],
   "source": [
    "household.loc[household['rez_esc-mean'].isnull(), ['r4t1', 'hogar_nin']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8440f5de8e7341aeac39743a48e85d4d1f5260f1"
   },
   "source": [
    "It looks like all the missing values in years behind may arise because there are no school-age children in the family. My guess is that this is the case and we will set all the missing values to zero in the years behind-derived columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce433cf99de1e098abe2c1281d9c51786f912c03"
   },
   "outputs": [],
   "source": [
    "years_behind = [x for x in household if x.startswith('rez')]\n",
    "household[years_behind] = household[years_behind].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e40eed11208a1aba12f51131c56db405ef2c869"
   },
   "source": [
    "The final missing values are those in mean education. For these, we can simply replace the missing `meaneduc` with the `escolari-mean`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9d3f14a30988dcf8cfd1640ce93cfeaef101e48"
   },
   "outputs": [],
   "source": [
    "household.loc[household['meaneduc'].isnull(), 'meaneduc'] = household.loc[household['meaneduc'].isnull(), 'escolari-mean']\n",
    "\n",
    "household.isnull().sum().sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5204e19043516637f329f34df73e225d022c3544"
   },
   "source": [
    "The only remaining missing values are in the rent column. We will leave these to be filled in via imputation. \n",
    "\n",
    "Now we can move on to the highly correlated variables. For each pair of variables with a correlation greater than 0.95, we'll remove one out of the pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7aa655559e58511a5bc6b28444a5f35a6ae4225"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = household.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print('There are {} collinear columns to remove.'.format(len(to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3cf38fd7c02b9cdd2efa8a7945542b25b2f68bae"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "sns.heatmap(corr_matrix.iloc[:20, :20], cmap = plt.cm.RdYlBu_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87121904543124f25bf651e76553241553ca72db"
   },
   "outputs": [],
   "source": [
    "household_selected = household[[x for x in household if x not in to_drop]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e98cdfbe378b280aa6594566f98834a55d36379b"
   },
   "source": [
    "Finally, we'll use the feature importances from a random forest to remove zero importance variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "021cbfe13233c3ff8eeaba3b23fdcbea4cb544c5"
   },
   "outputs": [],
   "source": [
    "# Extract the training data\n",
    "household_train = household_selected[household_selected['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "household_test = household_selected[household_selected['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "\n",
    "feature_names = list(household_train.columns)\n",
    "\n",
    "# Impute and scale\n",
    "household_train = pipeline.fit_transform(household_train)\n",
    "household_test = pipeline.transform(household_test)\n",
    "\n",
    "# Train a random forest model\n",
    "model = RandomForestClassifier(1000, random_state = 10)\n",
    "model.fit(household_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "39ed816e2cbca807e803b64a9f868dada8322fc0"
   },
   "outputs": [],
   "source": [
    "np.where(model.feature_importances_ == 0)[0]\n",
    "feature_names[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2019a4aacc635f78f6522953ae4f304185d3bcb1"
   },
   "outputs": [],
   "source": [
    "household = household.drop(columns = ['pisoother'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "530420c9251cbf153f958215afa18eee30b663f3"
   },
   "source": [
    "There are no features with 0 importance. Therefore we'll have to leave all the features at this point. Let's re-evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc1cb836dca322196a3ac62e7fa11fd33b47034c"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(100, random_state = 10)\n",
    "model_results = cv_model(household_train, train_labels, model, 'RF-S', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6986ec7fc099aec2477557fb2cf62f4ff9622170"
   },
   "source": [
    "The feature selection did not improve our model! We'll have to go back to our data before selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24c9cfa64878f984faca44b3920b281fc6e77ed6"
   },
   "source": [
    "# Advanced Modeling using the Gradient Boosting Machine\n",
    "\n",
    "Let's switch back from feature engineering / selection to more advanced modeling using the Gradient Boosting Machine (GBM). This model is extremely popular on Kaggle because of its great performance. The only issue with the GBM is the large number of hyperparameters, which can have a significant effect on model scores. For now, we'll use a set of hyperparameter values I've found work well for other problems (although there's no guarantee these will translate to this problem). Later we can work on hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8761f7135fecebbcf403059decb090cd8deb1a52"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1f0aba5a9af9ea0a81c4d31156ed9921283e82a"
   },
   "source": [
    "To choose the number of estimators (the number of decision trees in the ensemble, called `n_estimators` or `num_boost_rounds`), we'll use early stopping with 5-fold cross validation. This will keep adding estimators until the performance as measured by the Macro F1 Score has not increased for 100 training rounds. To use this metric, we'll have to define a custom metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e9af2c841c6d3f0047a5555b129b09285d7ee73"
   },
   "outputs": [],
   "source": [
    "def macro_f1_score(labels, predictions):\n",
    "    # Reshape the predictions as needed\n",
    "    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n",
    "    \n",
    "    metric_value = f1_score(labels, predictions, average = 'macro')\n",
    "    \n",
    "    # Return is name, value, is_higher_better\n",
    "    return 'macro_f1', metric_value, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f9803bfb46f116d1f910b04d955bbc3c4f11696"
   },
   "source": [
    "# Light Gradient Boosting Machine Implementation\n",
    "\n",
    "The function below implements training the gradient boosting machine with Stratified Kfold cross validation and early stopping to prevent overfitting to the training data (although this still does occur). The function performs training with cross validation and records the predictions for each fold. We can then take the mode prediction (breaking ties by whichever label was more common in the training data).\n",
    "\n",
    "We set the `n_estimators` to 10000 but we won't actually reach this number because we are using early stopping which will quit training estimators when the cross validation metric does not improve for `early_stopping_rounds`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ac8bebe40444e218362b385cc3ac408e7e70c84"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def model_gbm(features, labels, test_features, test_ids, nfolds = 5, return_preds = False):\n",
    "    \"\"\"Model using the GBM and cross validation.\n",
    "       Trains with early stopping on each fold.\n",
    "       Hyperparameters probably need to be tuned.\"\"\"\n",
    "    \n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Model with hyperparameters selected from previous work\n",
    "    model = lgb.LGBMClassifier(boosting_type = 'gbdt', n_estimators = 10000, max_depth = -1,\n",
    "                               learning_rate = 0.025, metric = 'None', min_child_samples = 30,\n",
    "                               reg_alpha = 0.35, reg_lambda = 0.6, num_leaves = 15, \n",
    "                               colsample_bytree = 0.85, objective = 'multiclass', \n",
    "                               class_weight = 'balanced', \n",
    "                               n_jobs = -1)\n",
    "    \n",
    "    # Using stratified kfold cross validation\n",
    "    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
    "    predictions = pd.DataFrame()\n",
    "    importances = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    labels = np.array(labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        # Dataframe for \n",
    "        fold_predictions = pd.DataFrame()\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(X_train, y_train, early_stopping_rounds = 100, \n",
    "                  eval_metric = macro_f1_score,\n",
    "                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_names = ['train', 'valid'],\n",
    "                  verbose = 200)\n",
    "        \n",
    "        # Record the validation fold score\n",
    "        valid_scores.append(model.best_score_['valid']['macro_f1'])\n",
    "        \n",
    "        # Make predictions from the fold\n",
    "        fold_probabilitites = model.predict_proba(test_features)\n",
    "        \n",
    "        # Record each prediction for each class as a column\n",
    "        for j in range(4):\n",
    "            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n",
    "            \n",
    "        fold_predictions['idhogar'] = test_ids\n",
    "        fold_predictions['fold'] = (i+1)\n",
    "        predictions = predictions.append(fold_predictions)\n",
    "        \n",
    "        importances += model.feature_importances_ / nfolds    \n",
    "\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                        'importance': importances})\n",
    "    valid_scores = np.array(valid_scores)\n",
    "    print(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n",
    "    \n",
    "    # If we want to examine predictions don't average over folds\n",
    "    if return_preds:\n",
    "        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "        return predictions, feature_importances\n",
    "    \n",
    "    # Average the predictions over folds\n",
    "    predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "    \n",
    "    # Find the class and associated probability\n",
    "    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "    predictions = predictions.drop(columns = ['fold'])\n",
    "    \n",
    "    # Merge with the base to have one prediction for each individual\n",
    "    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n",
    "        \n",
    "    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n",
    "    \n",
    "    # return the submission and feature importances\n",
    "    return submission, feature_importances, valid_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "996e3706f29a74cf566400b46ae3ce13d4d92b5d"
   },
   "source": [
    "Let's look at how this model works by returning the predictions rather than the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca12001cee144d03fa848885f6c17391524f0417"
   },
   "outputs": [],
   "source": [
    "# Extract the training data\n",
    "household_train = household[household['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "household_test = household[household['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "\n",
    "test_ids = list(household.loc[(household['Target'].isnull()), 'idhogar'])\n",
    "predictions, feature_importances = model_gbm(household_train, train_labels, household_test, test_ids, return_preds=True)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2832ef900fc10dec540cddad2db19c814f742056"
   },
   "source": [
    "The predictions contain the predictions for each fold. This means there might be cases where the predictions do not agree across folds. We can look at the confidence (probability) assigned to each prediction across folds. The kdeplots are colored by the value of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1603b695b71914f3ea20bd3d257f8276f34499cd"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 18\n",
    "g = sns.FacetGrid(predictions, row = 'fold', hue = 'Target', size = 3, aspect = 4)\n",
    "g.map(sns.kdeplot, 'confidence');\n",
    "g.add_legend();\n",
    "plt.suptitle('Distribution of Confidence by Fold and Color', y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4964e638cb27027c85200c679b93ec515bfccbae"
   },
   "source": [
    "We see that there are significant differences between folds. There appears to be the highest level of confidence for class 4 predictions which makes sense because this is the majority class. The model trains on many more examples of this class than any other and hence puts more probability on this class. This can potentially be corrected by oversampling the minority classes.\n",
    "\n",
    "We can look at the same data in a violinplot. This also shows the highest level of confidence for Target 4 with substantial differences between folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94be45b9ab2c39d3808153ad2878548c05cb379d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (24, 12))\n",
    "sns.violinplot(x = 'Target', y = 'confidence', hue = 'fold', data = predictions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77120aa6a8d93ddce0865f16298a9c145a8d3b1c"
   },
   "source": [
    "### Average Predictions over Folds\n",
    "\n",
    "To actually make one final prediction for each household, we want to average the predictions across folds. This means we have essentially built a bagging classifier by combining together predictions across folds. Each classifier trains on a different subset of the data, so this would be an example of bagging: bootstrap aggregating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b8049f123b56093eaac1d53fe84e26602c04595"
   },
   "outputs": [],
   "source": [
    "# Average the predictions over folds\n",
    "predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "\n",
    "# Find the class and associated probability\n",
    "predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "predictions = predictions.drop(columns = ['fold'])\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9c96f6e757732a9752baacba41d74521d607f4f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.boxplot(x = 'Target', y = 'confidence', data = predictions);\n",
    "plt.title('Confidence by Target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4aa82b1c58a305aa357821290abc205196fce66f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.violinplot(x = 'Target', y = 'confidence', data = predictions);\n",
    "plt.title('Confidence by Target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d7fa302dda5e1b4cd716784365395fecc455868"
   },
   "source": [
    "We see the same story: higher confidence for Target class 4. Overall, the levels of confidence in the predictions are not very high (I'm not sure if this is common for the Gradient Boosting Machine or an artifact of the small data size and imbalanced labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d342103dae17c506c1cf739e37c4c49253b3556e"
   },
   "outputs": [],
   "source": [
    "norm_lgb_fi = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b90d7a06237d731adf98fe08fc6237fd52b33ef"
   },
   "source": [
    "Now we can run the same code but return the submission dataframe. This has the predictions for every individual and is ready for uploading directly to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "120b749dc3a2650dfd025b909118b7f4debe0445"
   },
   "outputs": [],
   "source": [
    "submission, feature_importances, valid_scores_5 = model_gbm(household_train, train_labels, household_test, test_ids)\n",
    "submission.to_csv('lightgbm_baseline_5fold.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c2cc37332925c2e899a76005e2bd9b9438156d2"
   },
   "source": [
    "### Cross Validation with Early Stopping Notes\n",
    "\n",
    "Cross validation with early stopping is one of the most effective methods for preventing overfitting on the training set because it prevents us from continuing to add model complexity once it is clear that validation scores are not improving. Repeating this process across multiple folds helps to reduce the bias that comes from using a single fold. Early stopping also lets us train the model much quicker. Overall, __early stopping with cross validation__ is the best method to select the number of estimators in the Gradient Boosting Machine and should be our default technique when we desig an implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4b5ba8f595d796c54d8d218def9d7ec6cc6971e"
   },
   "outputs": [],
   "source": [
    "_ = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0711bf6af09c8f46b916f136cd73fa00ce41c1bc"
   },
   "source": [
    "__Education is still the most important feature.__ This is useful information that should be used, for example, to build more schools! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2afecf71934469b915b1be4490b485e0fc9826f3"
   },
   "outputs": [],
   "source": [
    "no_importance = list(feature_importances.loc[feature_importances[\"importance\"] == 0, 'feature'])\n",
    "print(f'There are {len(no_importance)} features with 0 importance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a48ad4eccd0f5523ece8f6f2781db38535044465"
   },
   "source": [
    "We'll make the model both with 5 and 10 folds of cross validation to compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a972e537a326c2736f256248b35d3263aa765403"
   },
   "outputs": [],
   "source": [
    "submission, feature_importances, valid_scores_10 = model_gbm(household_train, train_labels,\n",
    "                                                            household_test, test_ids, nfolds = 10)\n",
    "submission.to_csv('lgb_10fold_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "198345b21b5be4d037d2b952bff5fc52e1072159"
   },
   "source": [
    "### Modeling Results with Light GBM\n",
    "\n",
    "We'll compare all the models in a single chart making sure to also include the standard deviation across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f5d3b61641f67d5ad544f9ae552beede2aef61b"
   },
   "outputs": [],
   "source": [
    "model_results = model_results.append(pd.DataFrame({'model': [\"GBM_5Fold\", \"GBM_10Fold\"], \n",
    "                                                   'cv_mean': [valid_scores_5.mean(), valid_scores_10.mean()],\n",
    "                                                   'cv_std':  [valid_scores_5.std(), valid_scores_10.std()]}),\n",
    "                                    sort = True)\n",
    "\n",
    "model_results.set_index('model', inplace = True)\n",
    "model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n",
    "                                  yerr = list(model_results['cv_std']))\n",
    "plt.title('Model F1 Score Results');\n",
    "plt.ylabel('Mean F1 Score (with error bar)');\n",
    "model_results.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a03414c54570eccb3e3f7ebd37642061c67a2b4"
   },
   "source": [
    "The Gradient Boosting Machine has the highest average cross validation score but also the greatest variation between folds (I used 5 folds initially and 10 with the other models). There is still some performance to be extracted from this model with tuning (probably using random search).\n",
    "\n",
    "__The 5 Fold predictions score 0.427 when submitted. Different versions of this kernel might score different results, but this was the highest with the GBM.__\n",
    "\n",
    "The distribution of the labels still is skewed. One potential way we can address this is through oversampling the lower popularity classes.\n",
    "\n",
    "# Oversampling (in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fbc2733dce5c85a58ef292ba67cf95bb0ba30939"
   },
   "source": [
    "## More Feature Engineering / Feature Selection (in progress)\n",
    "\n",
    "We can use the variables already in the data to make new features. We'll take a look at this more later on, but for now let's make a simple feature: the range (`max` - `min`) of the ages of family members. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2b1265009304104e5bd3efbdd8e470633550dfa"
   },
   "outputs": [],
   "source": [
    "household['age-range'] = household['age-max'] - household['age-min']\n",
    "kde_target(household[household['Target'].notnull()], 'age-range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acbb188a97742c5c289a6f79f8e16cb3f7aba83b"
   },
   "outputs": [],
   "source": [
    "household['educ-range'] = household['escolari-max'] - household['escolari-min']\n",
    "kde_target(household[household['Target'].notnull()], 'educ-range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2892a480f24cf6a4afde8052e0e5db164f4582f0"
   },
   "outputs": [],
   "source": [
    "household['phones-per-capita'] = household['qmobilephone'] / household['tamviv']\n",
    "household['tablets-per-capita'] = household['v18q1'] / household['tamviv']\n",
    "household['rooms-per-capita'] = household['rooms'] / household['tamviv']\n",
    "household['rent-per-capita'] = household['v2a1'] / household['tamviv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "867c6344ae3031c152b04c52a2088a157605d936"
   },
   "source": [
    "For additional feature selection, we can remove any features that the Gradient Boosting Machine gave an importance of 0. These are different than those in the random forest (I think because the trees are shorter in the gradient boosting machine and hence there are fewer splits) with many more features having zero importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5366cc7a874d55c45f114bcd5615c62ea10fda5"
   },
   "outputs": [],
   "source": [
    "household = household[[x for x in household if x not in no_importance]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3074dcbe56739def364403986b59163aa7bd6098"
   },
   "source": [
    "We'll train one final model with the new set of features. This time we can use 5 folds for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd6541f58543bc64dce100491d688b3ee906366a"
   },
   "outputs": [],
   "source": [
    "# Extract the training data\n",
    "household_train = household[household['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "household_test = household[household['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "\n",
    "submission, feature_importances, valid_scores_adv = model_gbm(household_train, train_labels,\n",
    "                                                              household_test, test_ids, nfolds = 5)\n",
    "submission.to_csv('lgb_5adv_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7eb8b180f8b38c33db869c2012555d870f19be87"
   },
   "outputs": [],
   "source": [
    "model_results = model_results.append(pd.DataFrame({'model': [\"GBM_5ADV\"], \n",
    "                                                   'cv_mean': [valid_scores_adv.mean()],\n",
    "                                                   'cv_std':  [valid_scores_adv.std()]}),\n",
    "                                    sort = True)\n",
    "\n",
    "model_results.set_index('model', inplace = True)\n",
    "model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n",
    "                                  yerr = list(model_results['cv_std']))\n",
    "plt.title('Model F1 Score Results');\n",
    "plt.ylabel('Mean F1 Score (with error bar)');\n",
    "model_results.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97ae0039ad650fc9b3cc56766a1ae563157bb685"
   },
   "source": [
    "The final model has a _slightly_ lower average cross validation score with the 10 fold cross validation recording the overall best average score. However, it also has a greater standard deviation compared to 5 fold cv. It's possible the model is starting to overfit on some of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28b9f5962ed6a4e02014eded22f920e2ddff3739"
   },
   "outputs": [],
   "source": [
    "norm_fi = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1d7fef86b462b14a8b645ce9173cf5a45f22a5f"
   },
   "source": [
    "Some of the new features make it into the most important. However, these were derived from features that _already_ were important meaning we run the risk of overfitting to the training data. There is still some work that can be done with feature engineering! That is likely where the difference will be made in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "614761b3a7e675c661a22db8ff6a1c98e34d445c"
   },
   "outputs": [],
   "source": [
    "household.to_csv('household.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta_py import Boruta\n",
    "\n",
    "selector = Boruta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "680110aedf38c4954aba2ad91fcfdad4370869d3"
   },
   "source": [
    "# Conclusions and Next Steps\n",
    "\n",
    "In this notebook, we went through a step-by-step data science process on a real-world dataset\n",
    "\n",
    "1. Understand the problem\n",
    "2. Exploratory Data Analysis\n",
    "3. Feature Engineering\n",
    "    * Aggregate data \n",
    "    * Deal with missing values\n",
    "    * Preliminary feature engineering\n",
    "4. Model Selection\n",
    "    * Try many different models to see which one is most promising\n",
    "    * Hyperparameter tuning may also be included\n",
    "5. Feature selection \n",
    "6. Implementing best models\n",
    "\n",
    "From here, there are a number of aspects we need to work on:\n",
    "\n",
    "* Hyperparameter tuning: we did not spend much time optimizing the model! \n",
    "* Further feature selection: we probably do not need to retain all of the features to get the same performance\n",
    "* [Oversampling the minority class](http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html) / undersampling the majority class: an advanced techinque that might improve performance\n",
    "* Difference methods for dimensionality reduction such as Principal Components Analysis or [Uniform Manifold Approximation and Projection (UMAP)](https://github.com/lmcinnes/umap)\n",
    "* Examining and trying to explain _why_ our model makes predictions\n",
    "* Ensembling multiple models\n",
    "\n",
    "There are plenty of more opportunities for improvement in this compeition. I hope you'll join me along the way. My next kernel for this competition is available: [Featuretools for Good](https://www.kaggle.com/willkoehrsen/featuretools-for-good). It uses automated feature engineering to approach the problem.\n",
    "\n",
    "Best,\n",
    "\n",
    "Will \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9d93c507f54c2c9e95d3cb82e5221290c089565"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
